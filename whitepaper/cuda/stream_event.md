# ğŸ˜· CUDAæµå’Œäº‹ä»¶

### æµå’Œäº‹ä»¶çš„æ¦‚è¿°

CUDAæµæ˜¯ä¸€ç³»åˆ—å¼‚æ­¥çš„CUDAæ“ä½œï¼Œè¿™äº›æ“ä½œæŒ‰ç…§ä¸»æœºä»£ç ç¡®å®šçš„é¡ºåºåœ¨è®¾å¤‡ä¸Šæ‰§è¡Œã€‚æµèƒ½å°è£…è¿™äº›æ“ä½œï¼Œä¿æŒæ“ä½œçš„é¡ºåºï¼Œå…è®¸æ“ä½œåœ¨æµä¸­æ’é˜Ÿï¼Œå¹¶ä½¿å®ƒä»¬åœ¨å…ˆå‰çš„æ‰€æœ‰ æ“ä½œä¹‹åæ‰§è¡Œï¼Œå¹¶ä¸”å¯ä»¥æŸ¥è¯¢æ’é˜Ÿæ“ä½œçš„çŠ¶æ€ã€‚è¿™äº›æ“ä½œåŒ…æ‹¬åœ¨ä¸»æœºä¸è®¾å¤‡é—´è¿›è¡Œæ•°æ®ä¼  è¾“ï¼Œå†…æ ¸å¯åŠ¨ä»¥åŠå¤§å¤šæ•°ç”±ä¸»æœºå‘èµ·ä½†ç”±è®¾å¤‡å¤„ç†çš„å…¶ä»–å‘½ä»¤ã€‚æµä¸­æ“ä½œçš„æ‰§è¡Œç›¸å¯¹äºä¸» æœºæ€»æ˜¯å¼‚æ­¥çš„ã€‚`CUDA runtime`å†³å®šä½•æ—¶å¯ä»¥åœ¨è®¾å¤‡ä¸Šæ‰§è¡Œæ“ä½œã€‚<mark style="color:red;">æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯ä½¿ç”¨CUDA çš„APIæ¥ç¡®ä¿ä¸€ä¸ªå¼‚æ­¥æ“ä½œåœ¨è¿è¡Œç»“æœè¢«ä½¿ç”¨ä¹‹å‰å¯ä»¥å®Œæˆã€‚åœ¨åŒä¸€ä¸ªCUDAæµä¸­çš„æ“ä½œ æœ‰ä¸¥æ ¼çš„æ‰§è¡Œé¡ºåºï¼Œè€Œåœ¨ä¸åŒCUDAæµä¸­çš„æ“ä½œåœ¨æ‰§è¡Œé¡ºåºä¸Šä¸å—é™åˆ¶ã€‚</mark>ä½¿ç”¨å¤šä¸ªæµåŒæ—¶ å¯åŠ¨å¤šä¸ªå†…æ ¸ï¼Œå¯ä»¥å®ç°ç½‘æ ¼çº§å¹¶å‘ã€‚

å› ä¸ºæ‰€æœ‰åœ¨CUDAæµä¸­æ’é˜Ÿçš„æ“ä½œéƒ½æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥åœ¨ä¸»æœºä¸è®¾å¤‡ç³»ç»Ÿä¸­å¯ä»¥é‡å æ‰§ è¡Œå…¶ä»–æ“ä½œã€‚åœ¨åŒä¸€æ—¶é—´å†…å°†æµä¸­æ’é˜Ÿçš„æ“ä½œä¸å…¶ä»–æœ‰ç”¨çš„æ“ä½œä¸€èµ·æ‰§è¡Œï¼Œå¯ä»¥éšè—æ‰§è¡Œ é‚£äº›æ“ä½œçš„å¼€é”€ã€‚

**CUDAç¼–ç¨‹çš„ä¸€ä¸ªå…¸å‹æ¨¡å¼ï¼š**

1. å°†è¾“å…¥æ•°æ®ä»ä¸»æœºç§»åˆ°è®¾å¤‡ä¸Šã€‚&#x20;
2. åœ¨è®¾å¤‡ä¸Šæ‰§è¡Œä¸€ä¸ªå†…æ ¸ã€‚&#x20;
3. å°†ç»“æœä»è®¾å¤‡ç§»å›ä¸»æœºä¸­ã€‚

{% hint style="info" %}
ä»è½¯ä»¶çš„è§’åº¦æ¥çœ‹ï¼ŒCUDAæ“ä½œåœ¨ä¸åŒçš„æµä¸­å¹¶å‘è¿è¡Œï¼›è€Œä»ç¡¬ä»¶ä¸Šæ¥çœ‹ï¼Œä¸ä¸€å®šæ€» æ˜¯å¦‚æ­¤ã€‚æ ¹æ®PCIeæ€»çº¿äº‰ç”¨æˆ–æ¯ä¸ªSMèµ„æºçš„å¯ç”¨æ€§ï¼Œå®Œæˆä¸åŒçš„CUDAæµä»ç„¶éœ€è¦äº’ç›¸ç­‰å¾…ã€‚
{% endhint %}

### CUDAæµ

æ‰€æœ‰çš„CUDAæ“ä½œéƒ½åœ¨ä¸€ä¸ªæµä¸­æ˜¾å¼æˆ–éšå¼åœ°è¿è¡Œã€‚æµåˆ†ä¸º ä¸¤ç§ç±»å‹ï¼š

* éšå¼å£°æ˜çš„æµï¼ˆç©ºæµï¼‰

å¦‚æœæ²¡æœ‰æ˜¾å¼åœ°æŒ‡å®šä¸€ä¸ªæµï¼Œé‚£ä¹ˆå†…æ ¸å¯åŠ¨å’Œæ•°æ®ä¼ è¾“å°†é»˜è®¤ä½¿ç”¨ç©ºæµã€‚

* æ˜¾å¼å£°æ˜çš„æµï¼ˆéç©ºæµï¼‰&#x20;

éç©ºæµå¯ä»¥è¢«æ˜¾å¼åœ°åˆ›å»ºå’Œç®¡ç†ã€‚<mark style="color:red;">å¦‚æœæƒ³è¦é‡å ä¸åŒçš„CUDAæ“ä½œï¼Œå¿…é¡»ä½¿ç”¨éç©ºæµã€‚</mark>

<mark style="color:red;">åŸºäºæµçš„å¼‚æ­¥çš„å†…æ ¸å¯åŠ¨å’Œæ•°æ®ä¼ è¾“æ”¯æŒä»¥ä¸‹ç±»å‹çš„ç²—ç²’åº¦å¹¶å‘ï¼š</mark>&#x20;

* é‡å ä¸»æœºè®¡ç®—å’Œè®¾å¤‡è®¡ç®—&#x20;
* é‡å ä¸»æœºè®¡ç®—å’Œä¸»æœºä¸è®¾å¤‡é—´çš„æ•°æ®ä¼ è¾“&#x20;
* é‡å ä¸»æœºä¸è®¾å¤‡é—´çš„æ•°æ®ä¼ è¾“å’Œè®¾å¤‡è®¡ç®—&#x20;
* å¹¶å‘è®¾å¤‡è®¡ç®— &#x20;

æ€è€ƒä¸‹é¢ä½¿ç”¨é»˜è®¤æµçš„ä»£ç ï¼š

```c
cudaMemcpy(..., cudaMemcpyHostToDevice);
kernel<<<grid, block>>>(...);
cudaMemcpy(..., cudaMemcpyDeviceToHost);
```

è¦æƒ³ç†è§£ä¸€ä¸ªCUDAç¨‹åºï¼Œåº”è¯¥ä»è®¾å¤‡å’Œä¸»æœºä¸¤ä¸ªè§’åº¦å»è€ƒè™‘ã€‚**ä»è®¾å¤‡çš„è§’åº¦æ¥çœ‹ï¼Œ ä¸Šè¿°ä»£ç ä¸­æ‰€æœ‰çš„3ä¸ªæ“ä½œéƒ½è¢«å‘å¸ƒåˆ°é»˜è®¤çš„æµä¸­ï¼Œå¹¶ä¸”æŒ‰å‘å¸ƒé¡ºåºæ‰§è¡Œã€‚**è®¾å¤‡ä¸çŸ¥é“å…¶ ä»–è¢«æ‰§è¡Œçš„ä¸»æœºæ“ä½œã€‚ä»ä¸»æœºçš„è§’åº¦æ¥çœ‹ï¼Œæ¯ä¸ªæ•°æ®ä¼ è¾“éƒ½æ˜¯åŒæ­¥çš„ï¼Œåœ¨ç­‰å¾…å®ƒä»¬å®Œæˆæ—¶ï¼Œå°†å¼ºåˆ¶ä¸»æœºå¤„äºç©ºé—²çŠ¶æ€ã€‚å†…æ ¸å¯åŠ¨æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥æ— è®ºå†…æ ¸æ˜¯å¦å®Œæˆï¼Œä¸»æœºçš„åº”ç”¨ç¨‹åº å‡ ä¹éƒ½ç«‹å³æ¢å¤æ‰§è¡Œã€‚**è¿™ç§ç”±å†…æ ¸å¯åŠ¨å¯¼è‡´çš„å¼‚æ­¥è¡Œä¸ºå¯ä»¥ç”¨æ¥é‡å ä¸»æœºå’Œè®¾å¤‡çš„è®¡ç®—ã€‚**

æ•°æ®ä¼ è¾“ä¹Ÿå¯ä»¥è¢«å¼‚æ­¥å‘å¸ƒï¼Œä½†æ˜¯å¿…é¡»æ˜¾å¼åœ°è®¾ç½®ä¸€ä¸ªCUDAæµæ¥è£…è½½å®ƒä»¬ã€‚`CUDA runtime API`æä¾›äº†ä»¥ä¸‹`cudaMemcpy`å‡½æ•°çš„å¼‚æ­¥ç‰ˆæœ¬ï¼š

```c
cudaError_t cudaMemcpyAsync(void* dst, constvoid* src, size_t count, cudaMemcpyKind kind, cudaStream t stream =0)
```

è¯·æ³¨æ„é™„åŠ çš„æµæ ‡è¯†ç¬¦ä½œä¸ºç¬¬äº”ä¸ªå‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæµæ ‡è¯†ç¬¦è¢«è®¾ç½®ä¸ºé»˜è®¤æµã€‚è¿™ ä¸ªå‡½æ•°ä¸ä¸»æœºæ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥è°ƒç”¨åï¼Œæ§åˆ¶æƒå°†ç«‹å³è¿”å›åˆ°ä¸»æœºã€‚å°†æ•°æ®ä¼ è¾“æ“ä½œå’Œéç©ºæµè¿›è¡Œå…³è”æ˜¯å¾ˆå®¹æ˜“çš„ï¼Œä½†æ˜¯é¦–å…ˆéœ€è¦ä½¿ç”¨å¦‚ä¸‹ä»£ç åˆ›å»ºä¸€ä¸ªéç©ºæµï¼š

```c
cudaError_t cudaStreamCreate(cudaStream_t* pStream)
```

cudaStreamCreateåˆ›å»ºäº†ä¸€ä¸ªå¯ä»¥æ˜¾å¼ç®¡ç†çš„éç©ºæµã€‚ä¹‹åï¼Œè¿”å›åˆ°pStreamä¸­çš„æµå°± å¯ä»¥è¢«å½“ä½œæµå‚æ•°ä¾›cudaMemcpyAsyncå’Œå…¶ä»–å¼‚æ­¥CUDAçš„APIæ¥ä½¿ç”¨ã€‚<mark style="color:red;">åœ¨ä½¿ç”¨å¼‚æ­¥ CUDAå‡½æ•°æ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šä»å…ˆå‰å¯åŠ¨çš„å¼‚æ­¥æ“ä½œä¸­è¿”å›é”™è¯¯ä»£ç ã€‚</mark>

å½“æ‰§è¡Œå¼‚æ­¥æ•°æ®ä¼ è¾“æ—¶ï¼Œå¿…é¡»ä½¿ç”¨å›ºå®šï¼ˆæˆ–éåˆ†é¡µçš„ï¼‰ä¸»æœºå†…å­˜ã€‚å¯ä»¥ä½¿ç”¨`cudaMallocHost`å‡½æ•°æˆ–`cudaHostAlloc`å‡½æ•°åˆ†é…å›ºå®šå†…å­˜ï¼š

```c
cudaError t cudaMallocHost(void **ptr, sizet size);
cudaError_t cudaHostAlloc(void **pHost, size_t ssize, unsigned int flags);
```

<mark style="color:red;">åœ¨ä¸»æœºè™šæ‹Ÿå†…å­˜ä¸­è¿›è¡Œå›ºå®šåˆ†é…ï¼Œå¯ä»¥ç¡®ä¿å…¶åœ¨CPUå†…å­˜ä¸­çš„ç‰©ç†ä½ç½®åœ¨åº”ç”¨ç¨‹åºçš„æ•´ä¸ª ç”Ÿå‘½å‘¨æœŸä¸­ä¿æŒä¸å˜ã€‚å¦åˆ™ï¼Œæ“ä½œç³»ç»Ÿå¯ä»¥éšæ—¶è‡ªç”±æ”¹å˜ä¸»æœºè™šæ‹Ÿå†…å­˜çš„ç‰©ç†ä½ç½®ã€‚åœ¨æ²¡æœ‰å›ºå®šä¸»æœºå†…å­˜çš„æƒ…å†µä¸‹æ‰§è¡Œä¸€ä¸ªå¼‚æ­¥CUDAè½¬ç§»æ“ä½œï¼Œæ“ä½œç³»ç»Ÿå¯èƒ½ä¼šåœ¨ç‰©ç†å±‚é¢ ä¸Šç§»åŠ¨æ•°ç»„ï¼Œè¿™æ ·ä¼šå¯¼è‡´æœªå®šä¹‰çš„è¡Œä¸ºã€‚</mark>

{% hint style="info" %}
* `cudaMallocHost`åˆ†é…çš„æ˜¯é¡µé”å®šå†…å­˜ï¼Œä¹Ÿç§°ä¸ºå›ºå®šå†…å­˜ã€‚
* é¡µé”å®šå†…å­˜ä¸ä¼šè¢«åˆ†é¡µåˆ°ç£ç›˜ï¼Œå› æ­¤å¯¹äºGPUè®¿é—®éå¸¸é«˜æ•ˆã€‚
* åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œäººä»¬æ›´å–œæ¬¢ç›´æ¥ä½¿ç”¨`cudaMallocHost`æ¥åˆ†é…é¡µé”å®šå†…å­˜ï¼Œå› ä¸ºå®ƒæ›´å®¹æ˜“ä½¿ç”¨ã€‚
{% endhint %}

åœ¨éé»˜è®¤æµä¸­å¯åŠ¨å†…æ ¸ï¼Œå¿…é¡»åœ¨å†…æ ¸æ‰§è¡Œé…ç½®ä¸­æä¾›ä¸€ä¸ªæµæ ‡è¯†ç¬¦ä½œä¸ºç¬¬å››ä¸ªå‚æ•°ï¼š

```c
kernel name<<<grid, block, sharedMemSize, stream>>>(argument list);
```

ä¸€ä¸ªéé»˜è®¤æµå£°æ˜å¦‚ä¸‹ï¼š

```c
cudaStream_t stream;
```

éé»˜è®¤æµå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼è¿›è¡Œåˆ›å»ºï¼š

```c
cudaStreamCreate (&stream);
```

å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ä»£ç é‡Šæ”¾æµä¸­çš„èµ„æºï¼š

```c
cudaError t cudaStreamDestroy(cudaStream_t stream);
```

åœ¨ä¸€ä¸ªæµä¸­ï¼Œå½“cudaStreamDestroyå‡½æ•°è¢«è°ƒç”¨æ—¶ï¼Œå¦‚æœè¯¥æµä¸­ä»æœ‰æœªå®Œæˆçš„å·¥ä½œï¼Œ cudaStreamDestroyå‡½æ•°å°†ç«‹å³è¿”å›ï¼Œå½“æµä¸­æ‰€æœ‰çš„å·¥ä½œéƒ½å·²å®Œæˆæ—¶ï¼Œä¸æµç›¸å…³çš„èµ„æºå°†è¢« è‡ªåŠ¨é‡Šæ”¾ã€‚&#x20;

å› ä¸ºæ‰€æœ‰çš„CUDAæµæ“ä½œéƒ½æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥CUDAçš„APIæä¾›äº†ä¸¤ä¸ªå‡½æ•°æ¥æ£€æŸ¥æµä¸­æ‰€ æœ‰æ“ä½œæ˜¯å¦éƒ½å·²ç»å®Œæˆï¼š

```c
cudaError_t cudaStreamSynchronize(cudaStream_(t stream)
cudaError t cudaStreamQuery(cudaStream_t sttream);
```

<mark style="color:red;">**cudaStreamSynchronizeå¼ºåˆ¶é˜»å¡ä¸»æœºï¼Œç›´åˆ°åœ¨ç»™å®šæµä¸­æ‰€æœ‰çš„æ“ä½œéƒ½å®Œæˆäº†ã€‚cudaStreamQueryä¼šæ£€æŸ¥æµä¸­æ‰€æœ‰æ“ä½œæ˜¯å¦éƒ½å·²ç»å®Œæˆï¼Œä½†åœ¨å®ƒä»¬å®Œæˆå‰ä¸ä¼šé˜»å¡ä¸»æœºã€‚**</mark><mark style="color:red;">å½“æ‰€æœ‰æ“ä½œéƒ½å®Œæˆæ—¶cudaStreamQueryå‡½æ•°ä¼šè¿”å›cudaSuccessï¼Œå½“ä¸€ä¸ªæˆ–å¤šä¸ªæ“ä½œä»åœ¨æ‰§è¡Œæˆ–ç­‰å¾…æ‰§è¡Œæ—¶è¿”å›cudaErrorNotReadyã€‚</mark>

æ¥çœ‹å‡ ä¸ªç®€å•çš„ç¼–ç¨‹ç¤ºä¾‹ï¼š

```c
#include <cstdio>

using namespace std;

__global__ void
foo_kernel(int step)
{
    printf("loop: %d\n", step);
}

int main()
{
    int n_loop = 5;

    // ä½¿ç”¨é»˜è®¤æµ
    for (int i = 0; i < n_loop; i++)
        foo_kernel<<< 1, 1, 0, 0 >>>(i);

    cudaDeviceSynchronize();

    return 0;
}
```

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (4) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

```c
#include <cstdio>

using namespace std;

__global__ void
foo_kernel(int step)
{
    printf("loop: %d\n", step);
}

int main()
{
    int n_stream = 5;
    cudaStream_t *ls_stream;
    ls_stream = (cudaStream_t*) new cudaStream_t[n_stream];

    // create multiple streams
    for (int i = 0; i < n_stream; i++)
        cudaStreamCreate(&ls_stream[i]);

    // execute kernels with the CUDA stream each
    for (int i = 0; i < n_stream; i++)
        foo_kernel<<< 1, 1, 0, ls_stream[i] >>>(i);

    // synchronize the host and GPU
    cudaDeviceSynchronize();

    // terminates all the created CUDA streams
    for (int i = 0; i < n_stream; i++)
        cudaStreamDestroy(ls_stream[i]);
    delete [] ls_stream;

    return 0;
}
```

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (2) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

{% hint style="info" %}
<mark style="color:red;">ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œæ•°æ®ä¼ è¾“æ“ä½œè™½ç„¶åˆ†å¸ƒåœ¨ä¸åŒçš„æµä¸­ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰å¹¶å‘æ‰§è¡Œã€‚è¿™æ˜¯ç”±ä¸€ ä¸ªå…±äº«èµ„æºå¯¼è‡´çš„ï¼šPCIeæ€»çº¿ã€‚è™½ç„¶ä»ç¼–ç¨‹æ¨¡å‹çš„è§’åº¦æ¥çœ‹è¿™äº›æ“ä½œæ˜¯ç‹¬ç«‹çš„ï¼Œä½†æ˜¯å› ä¸ºå®ƒä»¬å…±äº«ä¸€ä¸ªç›¸åŒçš„ç¡¬ä»¶èµ„æºï¼Œæ‰€ä»¥å®ƒä»¬çš„æ‰§è¡Œå¿…é¡»æ˜¯ä¸²è¡Œçš„ã€‚</mark>
{% endhint %}

### æµåŒæ­¥

CUDA æµé€šè¿‡ cudaStreamSynchronize() å‡½æ•°æä¾›æµçº§åŒæ­¥ã€‚ä½¿ç”¨è¯¥å‡½æ•°å¯å¼ºåˆ¶ä¸»æœºç­‰å¾…æŸä¸ªæµæ“ä½œçš„ç»“æŸã€‚ä¸‹é¢çš„ä»£ç å±•ç¤ºäº†åœ¨å†…æ ¸æ‰§è¡Œç»“æŸæ—¶ä½¿ç”¨æµåŒæ­¥çš„ç¤ºä¾‹ï¼š

```c
#include <cstdio>

using namespace std;

__global__ void
foo_kernel(int step)
{
    printf("loop: %d\n", step);
}

int main()
{
    int n_stream = 5;
    cudaStream_t *ls_stream;
    ls_stream = (cudaStream_t*) new cudaStream_t[n_stream];

    // create multiple streams
    for (int i = 0; i < n_stream; i++)
        cudaStreamCreate(&ls_stream[i]);

    // execute kernels with the CUDA stream each
    for (int i = 0; i < n_stream; i++) {
       foo_kernel<<< 1, 1, 0, ls_stream[i] >>>(i);
       cudaStreamSynchronize(ls_stream[i]);
    }

    // synchronize the host and GPU
    cudaDeviceSynchronize();

    // terminates all the created CUDA streams
    for (int i = 0; i < n_stream; i++)
        cudaStreamDestroy(ls_stream[i]);
    delete [] ls_stream;

    return 0;
}
```

å†…æ ¸æ“ä½œçš„å¹¶å‘æ€§å°†å› åŒæ­¥è€Œæ¶ˆå¤±

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (3) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

å¯ä»¥çœ‹åˆ°ï¼Œæ‰€æœ‰å†…æ ¸æ‰§è¡Œéƒ½æ²¡æœ‰é‡å ç‚¹ï¼Œå°½ç®¡å®ƒä»¬æ˜¯ä»¥ä¸åŒçš„æµæ‰§è¡Œçš„ã€‚åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œæˆ‘ä»¬å¯ä»¥è®©ä¸»æœºç­‰å¾…æŸä¸€æµæ“ä½œè¿”å›çš„ç»“æœã€‚

### é»˜è®¤æµçš„ä½¿ç”¨

è¦åŒæ—¶è¿è¡Œå¤šä¸ªæµï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨æ˜¾å¼åˆ›å»ºçš„æµï¼Œå› ä¸º_<mark style="color:red;">**æ‰€æœ‰æµæ“ä½œéƒ½ä¸é»˜è®¤æµåŒæ­¥**</mark>_ï¼š

> In general, when an operation is issued to the NULL stream, the CUDA context waits on all operations previously issued to all blocking streams before starting that operation. Also, any operations issued to blocking streams will wait on preceding operations in the NULL stream to complete before executing.

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (4) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

ç›¸å…³ä»£ç å¦‚ä¸‹(åªéœ€è¦æ”¹å¾ªç¯å¯åŠ¨æ ¸å‡½æ•°é‚£å—ä»£ç )ï¼š

```c
//....çœç•¥
// execute kernels with the CUDA stream each
for (int i = 0; i < n_stream; i++)
    if (i == 3)
        foo_kernel<<< 1, 1, 0, 0 >>>(i);
    else
        foo_kernel<<< 1, 1, 0, ls_stream[i] >>>(i);
//....çœç•¥
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæœ€åä¸€æ¬¡æ“ä½œä¸èƒ½ä¸ä¹‹å‰çš„å†…æ ¸æ‰§è¡Œé‡å ï¼Œè€Œå¿…é¡»ç­‰åˆ°ç¬¬å››æ¬¡å†…æ ¸æ‰§è¡Œç»“æŸã€‚

### GPUæµæ°´çº¿æ‰§è¡Œ

å¤šæ•°æ®æµçš„ä¸»è¦ä¼˜åŠ¿ä¹‹ä¸€æ˜¯æ•°æ®ä¼ è¾“ä¸å†…æ ¸æ‰§è¡Œé‡å ã€‚é€šè¿‡é‡å å†…æ ¸æ“ä½œå’Œæ•°æ®ä¼ è¾“ï¼Œæˆ‘ä»¬å¯ä»¥éšè—æ•°æ®ä¼ è¾“å¼€é”€ï¼Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

> è¿™é‡Œçš„é‡å å…·ä½“è¯´çš„æ˜¯ï¼šå°†å¤§çš„æ•°æ®å—æ‹†åˆ†æˆå°å—ï¼Œå°†å¤šä¸ªH2D->Kernel->D2Hæ“ä½œæ”¾åˆ°å¤šä¸ªéé»˜è®¤æµä¸­æ‰§è¡Œã€‚

#### GPU æµæ°´çº¿æ¦‚å¿µ

å½“æˆ‘ä»¬æ‰§è¡Œå†…æ ¸å‡½æ•°æ—¶ï¼Œéœ€è¦å°†æ•°æ®ä»ä¸»æœºä¼ è¾“åˆ° GPUã€‚ ç„¶åï¼Œå†å°†ç»“æœä» GPU ä¼ è¾“å›ä¸»æœºã€‚ä¸‹å›¾æ˜¾ç¤ºäº†åœ¨ä¸»æœºå’Œå†…æ ¸ä¹‹é—´ä¼ è¾“æ•°æ®çš„è¿‡ç¨‹ï¼š

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

ç„¶è€Œï¼Œå†…æ ¸æ‰§è¡ŒåŸºæœ¬ä¸Šæ˜¯å¼‚æ­¥çš„ï¼Œä¸»æœºå’Œ GPU å¯ä»¥åŒæ—¶è¿è¡Œã€‚å¦‚æœä¸»æœºå’Œ GPU ä¹‹é—´çš„æ•°æ®ä¼ è¾“å…·æœ‰ç›¸åŒçš„ç‰¹æ€§ï¼Œæˆ‘ä»¬å°±å¯ä»¥é‡å æ‰§è¡Œã€‚

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (86).png" alt=""><figcaption></figcaption></figure>

ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸»æœºå’Œè®¾å¤‡ä¹‹é—´çš„æ•°æ®ä¼ è¾“å¯ä»¥ä¸å†…æ ¸æ‰§è¡Œé‡å ã€‚é‚£ä¹ˆï¼Œè¿™ç§é‡å æ“ä½œçš„å¥½å¤„å°±æ˜¯ç¼©çŸ­äº†åº”ç”¨ç¨‹åºçš„æ‰§è¡Œæ—¶é—´ã€‚é€šè¿‡æ¯”è¾ƒä¸¤å¹…å›¾çš„é•¿åº¦ï¼Œä½ å°±èƒ½ç¡®å®šå“ªç§æ“ä½œçš„ååé‡æ›´é«˜ã€‚

å…³äº CUDA æ•°æ®æµï¼Œæ‰€æœ‰ CUDA æ“ä½œï¼ˆæ•°æ®ä¼ è¾“å’Œå†…æ ¸æ‰§è¡Œï¼‰åœ¨åŒä¸€æ•°æ®æµä¸­éƒ½æ˜¯é¡ºåºè¿›è¡Œçš„ã€‚ä¸è¿‡ï¼Œè¿™äº›æ“ä½œå¯ä»¥ä¸ä¸åŒçš„æ•°æ®æµåŒæ—¶è¿›è¡Œã€‚ä¸‹å›¾æ˜¾ç¤ºäº†å¤šä¸ªæ•°æ®æµä¸­é‡å çš„æ•°æ®ä¼ è¾“å’Œå†…æ ¸æ“ä½œï¼š

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (87).png" alt="" width="563"><figcaption></figcaption></figure>

<mark style="color:red;">è¦å®ç°è¿™ç§æµæ°´çº¿æ“ä½œï¼Œæœ‰ä¸‰ä¸ªæ¡ä»¶ï¼š</mark>

1. ä¸»æœºå†…å­˜åº”è¢«åˆ†é…ä¸º`pinned memory`(cudaMallocHost() å‡½æ•°å’Œ cudaFreeHost() å‡½æ•°)ã€‚
2. åœ¨ä¸»æœºå’Œ GPU ä¹‹é—´ä¼ è¾“æ•°æ®è€Œä¸é˜»å¡ä¸»æœºè¿›ç¨‹(cudaMemcpyAsync() å‡½æ•°)ã€‚
3. å°†æ¯ä¸ªæ“ä½œæ”¾åˆ°ä¸åŒçš„ CUDA æµä¸­ç®¡ç†ï¼Œä»¥å®ç°å¹¶å‘æ“ä½œã€‚

#### æ„å»ºæµæ°´çº¿æ‰§è¡Œ

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªåœ¨æ•°æ®ä¼ è¾“å’Œå†…æ ¸æ‰§è¡Œä¸­è¿›è¡Œæµæ°´çº¿æ“ä½œçš„åº”ç”¨ç¨‹åºã€‚åœ¨æ­¤åº”ç”¨ç¨‹åºä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå†…æ ¸å‡½æ•°ï¼Œé€šè¿‡å¯¹æ•°æ®æµè¿›è¡Œåˆ†ç‰‡(åˆ†æˆ4æ®µ)ï¼Œå°†ä¸¤ä¸ªå‘é‡ç›¸åŠ ï¼Œç„¶åè¾“å‡ºç»“æœã€‚ æ¯ä¸ªæ ¸å‡½æ•°ä¸­å°†é‡å¤åŠ æ³•æ“ä½œ 500 æ¬¡ï¼Œä»¥å»¶é•¿å†…æ ¸æ‰§è¡Œæ—¶é—´ã€‚å› æ­¤ï¼Œå®ç°çš„ä»£ç å¦‚ä¸‹ï¼š

```c
__global__ void
vecAdd_kernel(float *c, const float* a, const float* b)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    for (int i = 0; i < 500; i++)
        c[idx] = a[idx] + b[idx];
}
```

ä¸ºäº†å¤„ç†æ¯ä¸ªæµçš„æ“ä½œï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®¡ç† CUDA æµå’Œ CUDA æ“ä½œçš„ç±»ã€‚ä½¿ç”¨è¯¥ç±»ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•æ¥ç®¡ç† CUDA æµã€‚ä»¥ä¸‹ä»£ç æ˜¾ç¤ºäº†è¯¥ç±»çš„åŸºæœ¬æ¶æ„ï¼š

```c
class Operator
{
private:
    int index;
    cudaStream_t stream;

public:
    Operator() {
        cudaStreamCreate(&stream);
    }

    ~Operator() {
        cudaStreamDestroy(stream);
    }

    void set_index(int idx) { index = idx; }
    void async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize);

}; // Operator

void Operator::async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize)
{
    // copy host -> device
    cudaMemcpyAsync(d_a, h_a, bufsize, cudaMemcpyHostToDevice, stream);
    cudaMemcpyAsync(d_b, h_b, bufsize, cudaMemcpyHostToDevice, stream);

    // launch cuda kernel
    dim3 dimBlock(256);
    dim3 dimGrid(size / dimBlock.x);
    vecAdd_kernel<<< dimGrid, dimBlock, 0, stream >>>(d_c, d_a, d_b);

    // copy device -> host
    cudaMemcpyAsync(h_c, d_c, bufsize, cudaMemcpyDeviceToHost, stream);

    cudaStreamSynchronize(stream);

    printf("Launched GPU task %d\n", index);
}
```

å®Œæ•´çš„ä»£ç å¦‚ä¸‹ï¼š

```c
#include <cstdio>
#include <helper_timer.h>

using namespace std;

__global__ void vecAdd_kernel(float *c, const float* a, const float* b);
void init_buffer(float *data, const int size);

class Operator
{
private:
    int index;
    cudaStream_t stream;

public:
    Operator() {
        cudaStreamCreate(&stream);
    }

    ~Operator() {
        cudaStreamDestroy(stream);
    }

    void set_index(int idx) { index = idx; }
    void async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize);

}; // Operator

void Operator::async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize)
{
    // copy host -> device
    cudaMemcpyAsync(d_a, h_a, bufsize, cudaMemcpyHostToDevice, stream);
    cudaMemcpyAsync(d_b, h_b, bufsize, cudaMemcpyHostToDevice, stream);

    // launch cuda kernel
    dim3 dimBlock(256);
    dim3 dimGrid(size / dimBlock.x);
    vecAdd_kernel<<< dimGrid, dimBlock, 0, stream >>>(d_c, d_a, d_b);

    // copy device -> host
    cudaMemcpyAsync(h_c, d_c, bufsize, cudaMemcpyDeviceToHost, stream);

    printf("Launched GPU task %d\n", index);
}

int main(int argc, char* argv[])
{
    float *h_a, *h_b, *h_c;
    float *d_a, *d_b, *d_c;
    int size = 1 << 24;
    int bufsize = size * sizeof(float);
    int num_operator = 4;

    if (argc != 1)
        num_operator = atoi(argv[1]);
    
    // allocate host memories
    cudaMallocHost((void**)&h_a, bufsize);
    cudaMallocHost((void**)&h_b, bufsize);
    cudaMallocHost((void**)&h_c, bufsize);

    // initialize host values
    srand(8215);
    init_buffer(h_a, size);
    init_buffer(h_b, size);
    init_buffer(h_c, size);

    // allocate device memories
    cudaMalloc((void**)&d_a, bufsize);
    cudaMalloc((void**)&d_b, bufsize);
    cudaMalloc((void**)&d_c, bufsize);

    // create list of operation elements
    Operator *ls_operator = new Operator[num_operator];

    // initialize & start timer
    StopWatchInterface *timer;
    sdkCreateTimer(&timer);
    sdkStartTimer(&timer);

    // execute each operator collesponding data
    for (int i = 0; i < num_operator; i++) {
        int offset = i * size / num_operator;
        ls_operator[i].set_index(i);
        ls_operator[i].async_operation(&h_c[offset], &h_a[offset], &h_b[offset],
                                       &d_c[offset], &d_a[offset], &d_b[offset],
                                       size / num_operator, bufsize / num_operator);
    }

    // synchronize until all the stream operation is finished
    cudaDeviceSynchronize();

    sdkStopTimer(&timer);

    // print out the result
    int print_idx = 256;
    printf("compared a sample result...\n");
    printf("host: %.6f, device: %.6f\n",  h_a[print_idx] + h_b[print_idx], h_c[print_idx]);

    // Compute and print the performance
    float elapsed_time_msed = sdkGetTimerValue(&timer);
    float bandwidth = 3 * bufsize * sizeof(float) / elapsed_time_msed / 1e6;
    printf("Time= %.3f msec, bandwidth= %f GB/s\n", elapsed_time_msed, bandwidth);

    sdkDeleteTimer(&timer);

    // terminate operators
    delete [] ls_operator;

    // terminate device memories
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    // terminate host memories
    cudaFreeHost(h_a);
    cudaFreeHost(h_b);
    cudaFreeHost(h_c);
    
    return 0;
}

void init_buffer(float *data, const int size)
{
    for (int i = 0; i < size; i++) 
        data[i] = rand() / (float)RAND_MAX;
}

__global__ void
vecAdd_kernel(float *c, const float* a, const float* b)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    for (int i = 0; i < 500; i++)
        c[idx] = a[idx] + b[idx];
}
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```bash
 Launched GPU task 0
 Launched GPU task 1
 Launched GPU task 2
 Launched GPU task 3
 compared a sample result...
 host: 1.523750, device: 1.523750
 Time= 29.508 msec, bandwidth= 27.291121 GB/s
```

ä¸‹é¢çš„æˆªå›¾æ˜¾ç¤ºäº†å››ä¸ª CUDA æ•°æ®æµé€šè¿‡é‡å æ•°æ®ä¼ è¾“å’Œå†…æ ¸æ‰§è¡Œè¿›è¡Œçš„æ“ä½œï¼š

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (88).png" alt=""><figcaption></figcaption></figure>

å› æ­¤ï¼ŒGPU å¯ä»¥ä¸€ç›´å¿™åˆ°æœ€åä¸€ä¸ªå†…æ ¸æ‰§è¡Œå®Œæ¯•ï¼Œè€Œä¸”æˆ‘ä»¬å¯ä»¥éšè—å¤§éƒ¨åˆ†æ•°æ®ä¼ è¾“ã€‚è¿™ä¸ä»…æé«˜äº† GPU çš„åˆ©ç”¨ç‡ï¼Œè¿˜ç¼©çŸ­äº†åº”ç”¨ç¨‹åºæ€»çš„æ‰§è¡Œæ—¶é—´ã€‚

åœ¨å†…æ ¸å‡½æ•°æ‰§è¡Œä¹‹é—´ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œè™½ç„¶å®ƒä»¬å±äºä¸åŒçš„ CUDA æµï¼Œä½†æ˜¯å­˜åœ¨å¾ç”¨çª—å£æœŸã€‚è¿™æ˜¯å› ä¸º GPU è°ƒåº¦å™¨é¦–å…ˆä¸ºç¬¬ä¸€ä¸ªè¯·æ±‚æä¾›æœåŠ¡ã€‚ä¸è¿‡ï¼Œå½“ä»»åŠ¡å®Œæˆåï¼Œæµå¼å¤šå¤„ç†å™¨å°±ä¸ºå¦ä¸€ä¸ª CUDA æµä¸­çš„å†…æ ¸æä¾›æœåŠ¡ã€‚

åœ¨æ‰€æœ‰çš„ CUDA æµæ“ä½œç»“æŸåï¼Œæˆ‘ä»¬éœ€è¦åŒæ­¥ä¸»æœºå’Œ GPUè®¾å¤‡ï¼Œä»¥ç¡®è®¤ GPU ä¸Šçš„æ‰€æœ‰ CUDA æ“ä½œéƒ½å·²å®Œæˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨å¾ªç¯ä¹‹åä½¿ç”¨äº† `cudaDeviceSynchronize()`ã€‚è¯¥å‡½æ•°å¯ä»¥åœ¨å‡½æ•°è°ƒç”¨å¤„åŒæ­¥æ‰€æœ‰çš„ GPU æ“ä½œã€‚

<mark style="color:red;">å¯¹äºåŒæ­¥ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„ä»£ç æ›¿æ¢</mark> <mark style="color:red;"></mark><mark style="color:red;">`cudaDeviceSynchronize()`</mark><mark style="color:red;">å‡½æ•°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿˜å¿…é¡»å°†ç§æœ‰æˆå‘˜</mark> <mark style="color:red;"></mark><mark style="color:red;">`_stream`</mark> <mark style="color:red;"></mark><mark style="color:red;">æ”¹ä¸ºå…¬æœ‰</mark>ï¼š

```c
 for (int i = 0; i < num_operator; i++) {
    cudaStreamSynchronize(ls_operator[i]._stream);
 }
```

<mark style="color:red;">å¦‚æœæ¯ä¸ªæµæ“ä½œå®Œæˆåç«‹å³æ‰§è¡Œä¸€äº›ç‰¹å®šçš„CPUæ“ä½œï¼Œé‚£ä¹ˆè¿™ç§è®¾è®¡å¯èƒ½ä¼šå¯¼è‡´ä¸å¿…è¦çš„åŒæ­¥ç‚¹ï¼Œä»è€Œå½±å“ç¨‹åºçš„æ•´ä½“æ€§èƒ½ã€‚è¿™æ˜¯å› ä¸ºå³ä½¿æŸäº›æµçš„æ“ä½œå·²ç»å®Œæˆï¼ŒCPUä¸Šçš„åç»­æ“ä½œå¯èƒ½ä»ç„¶éœ€è¦ç­‰å¾…å…¶ä»–æµçš„æ“ä½œå®Œæˆæ‰èƒ½å¼€å§‹ã€‚(è¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡æµå›è°ƒå‡½æ•°çš„æ–¹å¼æ¯”è¾ƒå®Œç¾çš„è§£å†³)</mark>

å¦‚æœåœ¨å‡½æ•°`async_operation`ä¸­ä½¿ç”¨ `cudaStreamSynchronize()` è¿™å°†æ— æ³•é‡å æ ¸å‡½æ•°çš„æ‰§è¡Œä¸æ•°æ®çš„ä¼ è¾“ã€‚

```c
void Operator::async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize)
{
    // copy host -> device
    cudaMemcpyAsync(d_a, h_a, bufsize, cudaMemcpyHostToDevice, stream);
    cudaMemcpyAsync(d_b, h_b, bufsize, cudaMemcpyHostToDevice, stream);

    // launch cuda kernel
    dim3 dimBlock(256);
    dim3 dimGrid(size / dimBlock.x);
    vecAdd_kernel<<< dimGrid, dimBlock, 0, stream >>>(d_c, d_a, d_b);

    // copy device -> host
    cudaMemcpyAsync(h_c, d_c, bufsize, cudaMemcpyDeviceToHost, stream);

    cudaStreamSynchronize(stream);

    printf("Launched GPU task %d\n", index);
}
```

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (89).png" alt=""><figcaption></figcaption></figure>

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæµ‹å¾—çš„æ‰§è¡Œæ—¶é—´ä¸º 41.521 æ¯«ç§’ï¼Œæ¯”é‡å æ‰§è¡Œæ—¶é—´æ…¢äº†çº¦ 40%ã€‚

### æµå›è°ƒå‡½æ•°

<mark style="color:red;">CUDA å›è°ƒå‡½æ•°æ˜¯ç”± GPU ä¸Šä¸‹æ–‡æ‰§è¡Œçš„å¯è°ƒç”¨ä¸»æœºå‡½æ•°ã€‚åˆ©ç”¨å®ƒï¼Œç¨‹åºå‘˜å¯ä»¥åœ¨ GPU æ“ä½œç»“æŸä¹‹åè°ƒç”¨æ‰€éœ€çš„ä¸»æœºæ“ä½œã€‚</mark>

CUDA å›è°ƒå‡½æ•°æœ‰ä¸€ä¸ªåä¸º `CUDART_CB` çš„ç‰¹æ®Šæ•°æ®ç±»å‹ã€‚ä½¿ç”¨è¿™ç§ç±»å‹ï¼Œç¨‹åºå‘˜å¯ä»¥æŒ‡å®šç”±å“ªä¸ª CUDA æµå¯åŠ¨è¯¥å‡½æ•°ã€ä¼ é€’ GPU é”™è¯¯çŠ¶æ€å¹¶æä¾›ç”¨æˆ·æ•°æ®ã€‚

ä¸ºæ³¨å†Œå›è°ƒå‡½æ•°ï¼ŒCUDA æä¾›äº† `cudaStreamAddCallback()`ã€‚è¯¥å‡½æ•°æ¥å— CUDA æµã€CUDA å›è°ƒå‡½æ•°åŠå…¶å‚æ•°ï¼Œè¿™æ ·å°±å¯ä»¥ä»æŒ‡å®šçš„ CUDA æµè°ƒç”¨æŒ‡å®šçš„ CUDA å›è°ƒå‡½æ•°ï¼Œè·å–ç”¨æˆ·æ•°æ®ã€‚è¯¥å‡½æ•°æœ‰å››ä¸ªè¾“å…¥å‚æ•°ï¼Œä½†æœ€åä¸€ä¸ªæ˜¯ä¿ç•™å‚æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨è¯¥å‚æ•°ï¼Œå…¶å€¼ä¸º 0ã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ”¹è¿›ä»£ç ï¼Œä½¿ç”¨å›è°ƒå‡½æ•°å¹¶è¾“å‡ºå•ä¸ªæ•°æ®æµçš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œå°†ä¸‹é¢è¿™äº›å£°æ˜æ”¾å…¥åˆ°`Operator`ç±»çš„`private`åŒºåŸŸï¼š

```c
private:
    int _index;
    cudaStream_t stream;
    StopWatchInterface *p_timer;

    static void CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData);
    void print_time();
```

`Callback()` å‡½æ•°å°†åœ¨æ¯ä¸ªæ•°æ®æµæ“ä½œå®Œæˆåè°ƒç”¨ï¼Œè€Œ `print_time()` å‡½æ•°å°†ä½¿ç”¨ä¸»æœºç«¯(`host side`)è®¡æ—¶å™¨ `_p_timer` æŠ¥å‘Šä¼°è®¡çš„æ€§èƒ½ã€‚è¿™äº›å‡½æ•°çš„å®ç°å¦‚ä¸‹ï¼š

```c
void Operator::CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData) {
    Operator* this_ = (Operator*) userData;
    this_->print_time();
}

void Operator::print_time() {
    sdkStopTimer(&p_timer);    // end timer
    float elapsed_time_msed = sdkGetTimerValue(&p_timer);
    printf("stream %2d - elapsed %.3f ms \n", _index, elapsed_time_msed);
}
```

å®Œæ•´çš„ç¨‹åºå¦‚ä¸‹ï¼š

```c
#include <cstdio>
#include <helper_timer.h>

using namespace std;

__global__ void vecAdd_kernel(float *c, const float* a, const float* b);
void init_buffer(float *data, const int size);

class Operator
{
private:
    int _index;
    cudaStream_t stream;
    StopWatchInterface *p_timer;

    static void CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData);
    void print_time();

public:
    Operator() {
        cudaStreamCreate(&stream);
        sdkCreateTimer(&p_timer);
    }

    ~Operator() {
        cudaStreamDestroy(stream);
        sdkDeleteTimer(&p_timer);
    }

    void set_index(int index) { _index = index; }
    void async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize);

}; // Operator

void Operator::CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData) {
    Operator* this_ = (Operator*) userData;
    this_->print_time();
}

void Operator::print_time() {
    sdkStopTimer(&p_timer);    // end timer
    float elapsed_time_msed = sdkGetTimerValue(&p_timer);
    printf("stream %2d - elapsed %.3f ms \n", _index, elapsed_time_msed);
}

void Operator::async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize)
{
    // start timer
    sdkStartTimer(&p_timer);

    // copy host -> device
    cudaMemcpyAsync(d_a, h_a, bufsize, cudaMemcpyHostToDevice, stream);
    cudaMemcpyAsync(d_b, h_b, bufsize, cudaMemcpyHostToDevice, stream);

    // launch cuda kernel
    dim3 dimBlock(256);
    dim3 dimGrid(size / dimBlock.x);
    vecAdd_kernel<<< dimGrid, dimBlock, 0, stream >>>(d_c, d_a, d_b);

    // copy device -> host
    cudaMemcpyAsync(h_c, d_c, bufsize, cudaMemcpyDeviceToHost, stream);

    // register callback function
    cudaStreamAddCallback(stream, Operator::Callback, this, 0);
}

int main(int argc, char* argv[])
{
    float *h_a, *h_b, *h_c;
    float *d_a, *d_b, *d_c;
    int size = 1 << 24;
    int bufsize = size * sizeof(float);
    int num_operator = 4;

    if (argc != 1)
        num_operator = atoi(argv[1]);

    // initialize timer
    StopWatchInterface *timer;
    sdkCreateTimer(&timer);
    
    // allocate host memories
    cudaMallocHost((void**)&h_a, bufsize);
    cudaMallocHost((void**)&h_b, bufsize);
    cudaMallocHost((void**)&h_c, bufsize);

    // initialize host values
    srand(2019);
    init_buffer(h_a, size);
    init_buffer(h_b, size);
    init_buffer(h_c, size);

    // allocate device memories
    cudaMalloc((void**)&d_a, bufsize);
    cudaMalloc((void**)&d_b, bufsize);
    cudaMalloc((void**)&d_c, bufsize);

    // create list of operation elements
    Operator *ls_operator = new Operator[num_operator];

    sdkStartTimer(&timer);
    
    // execute each operator collesponding data
    for (int i = 0; i < num_operator; i++) {
        int offset = i * size / num_operator;
        ls_operator[i].set_index(i);
        ls_operator[i].async_operation(&h_c[offset], &h_a[offset], &h_b[offset],
                                       &d_c[offset], &d_a[offset], &d_b[offset],
                                       size / num_operator, bufsize / num_operator);
    }

    // synchronize all the stream operation
    cudaDeviceSynchronize();

    sdkStopTimer(&timer);

    // print out the result
    int print_idx = 256;
    printf("compared a sample result...\n");
    printf("host: %.6f, device: %.6f\n",  h_a[print_idx] + h_b[print_idx], h_c[print_idx]);

    // Compute and print the performance
    float elapsed_time_msed = sdkGetTimerValue(&timer);
    float bandwidth = 3 * bufsize * sizeof(float) / elapsed_time_msed / 1e6;
    printf("Time= %.3f msec, bandwidth= %f GB/s\n", elapsed_time_msed, bandwidth);

    // delete timer
    sdkDeleteTimer(&timer);

    // terminate operators
    delete [] ls_operator;

    // terminate device memories
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    // terminate host memories
    cudaFreeHost(h_a);
    cudaFreeHost(h_b);
    cudaFreeHost(h_c);
    
    return 0;
}

__global__ void
vecAdd_kernel(float *c, const float* a, const float* b)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    for (int i = 0; i < 500; i++)
        c[idx] = a[idx] + b[idx];
}

void init_buffer(float *data, const int size)
{
    for (int i = 0; i < size; i++) 
        data[i] = rand() / (float)RAND_MAX;
}
```

ç¨‹åºè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```bash
stream 0 - elapsed 11.136 ms
stream 1 - elapsed 16.998 ms
stream 2 - elapsed 23.283 ms
stream 3 - elapsed 29.487 ms
compared a sample result...
host: 1.523750, device: 1.523750
Time= 29.771 msec, bandwidth= 27.050028 GB/
```

å¯ä»¥çœ‹å‡ºæµå›è°ƒå‡½æ•°å¯ä»¥ç›´æ¥é¢„æµ‹æ ¸å‡½æ•°çš„æ‰§è¡Œæ—¶é—´ï¼Œä½†æ˜¯ç”±äºå­˜åœ¨`overlap`ï¼Œä¼šå¯¼è‡´é åçš„CUDAæµæ‰§è¡Œæ—¶é—´æ¯”è¾ƒé•¿

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (90).png" alt=""><figcaption></figcaption></figure>

### æµçš„ä¼˜å…ˆçº§

æˆ‘ä»¬ä» Operator ç±»ä¸­åˆ›å»ºä¸€ä¸ªæ´¾ç”Ÿç±»ï¼Œå®ƒå°†å¤„ç†æµçš„ä¼˜å…ˆçº§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æˆå‘˜å˜é‡ stream çš„ä¿æŠ¤çº§åˆ«ä»ç§æœ‰æˆå‘˜æ”¹ä¸ºå—ä¿æŠ¤æˆå‘˜ã€‚æ­¤å¤–ï¼Œæ„é€ å‡½æ•°å¯ä»¥é€‰æ‹©æ€§åœ°åˆ›å»ºæµï¼Œå› ä¸ºè¿™å¯ä»¥ç”±æ´¾ç”Ÿç±»å®Œæˆã€‚å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

```c
#include <cstdio>
#include <helper_timer.h>

using namespace std;

__global__ void vecAdd_kernel(float *c, const float* a, const float* b);
void init_buffer(float *data, const int size);

class Operator
{
private:
    int _index;
    StopWatchInterface *p_timer;

    static void CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData);
    void print_time();

protected:
    cudaStream_t stream = nullptr;

public:
    Operator(bool create_stream = true) {
        if (create_stream)
            cudaStreamCreate(&stream);
        sdkCreateTimer(&p_timer);
    }

    ~Operator() {
        if (stream != nullptr)
            cudaStreamDestroy(stream);
        sdkDeleteTimer(&p_timer);
    }

    void set_index(int index) { _index = index; }
    void async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize);

}; // Operator

void Operator::CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData) {
    Operator* this_ = (Operator*) userData;
    this_->print_time();
}

void Operator::print_time() {
    // end timer
    sdkStopTimer(&p_timer);
    float elapsed_time_msed = sdkGetTimerValue(&p_timer);
    printf("stream %2d - elapsed %.3f ms \n", _index, elapsed_time_msed);
}

void Operator::async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize)
{
    // start timer
    sdkStartTimer(&p_timer);

    // copy host -> device
    cudaMemcpyAsync(d_a, h_a, bufsize, cudaMemcpyHostToDevice, stream);
    cudaMemcpyAsync(d_b, h_b, bufsize, cudaMemcpyHostToDevice, stream);

    // launch cuda kernel
    dim3 dimBlock(256);
    dim3 dimGrid(size / dimBlock.x);
    vecAdd_kernel<<< dimGrid, dimBlock, 0, stream >>>(d_c, d_a, d_b);

    // copy device -> host
    cudaMemcpyAsync(h_c, d_c, bufsize, cudaMemcpyDeviceToHost, stream);

    // register callback function
    cudaStreamAddCallback(stream, Operator::Callback, this, 0);
}

class Operator_with_priority: public Operator {
public:
    Operator_with_priority() : Operator(false) {}

    void set_priority(int priority) {
        // ç”¨äºåˆ›å»ºä¸€ä¸ªæ–°çš„ CUDA æµï¼ˆstreamï¼‰ï¼Œå¹¶ä¸”ä¸ºè¿™ä¸ªæµè®¾ç½®ä¸€ä¸ªä¼˜å…ˆçº§
        cudaStreamCreateWithPriority(&stream, cudaStreamNonBlocking, priority);
    }
};

int main(int argc, char* argv[])
{
    float *h_a, *h_b, *h_c;
    float *d_a, *d_b, *d_c;
    int size = 1 << 24;
    int bufsize = size * sizeof(float);
    int num_operator = 4;

    if (argc != 1)
        num_operator = atoi(argv[1]);

    // check the current device supports CUDA stream's prority
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, 0); 
    if (prop.streamPrioritiesSupported == 0) {
        printf("This device does not support priority streams");
        return 1;
    }

    // initialize timer
    StopWatchInterface *timer;
    sdkCreateTimer(&timer);

    // allocate host memories
    cudaMallocHost((void**)&h_a, bufsize);
    cudaMallocHost((void**)&h_b, bufsize);
    cudaMallocHost((void**)&h_c, bufsize);

    // initialize host values
    srand(2019);
    init_buffer(h_a, size);
    init_buffer(h_b, size);
    init_buffer(h_c, size);

    // allocate device memories
    cudaMalloc((void**)&d_a, bufsize);
    cudaMalloc((void**)&d_b, bufsize);
    cudaMalloc((void**)&d_c, bufsize);

    // create list of operation elements
    Operator_with_priority *ls_operator = new Operator_with_priority[num_operator];

    // Get Priority range
    int priority_low, priority_high;
    cudaDeviceGetStreamPriorityRange(&priority_low, &priority_high);
    printf("Priority Range: low(%d), high(%d)\n", priority_low, priority_high);

    // start to measure the execution time
    sdkStartTimer(&timer);
    
    // execute each operator corresponding data
    // priority setting for each CUDA stream
    for (int i = 0; i < num_operator; i++) {
        // int offset = i * size / num_operator;
        ls_operator[i].set_index(i);
        if (i + 1 == num_operator)
            ls_operator[i].set_priority(priority_high);
        else
            ls_operator[i].set_priority(priority_low);
    }

    // operation (copy(H2D), kernel execution, copy(D2H))
    for (int i = 0; i < num_operator; i++) {
        int offset = i * size / num_operator;
        ls_operator[i].async_operation(&h_c[offset], &h_a[offset], &h_b[offset],
                                       &d_c[offset], &d_a[offset], &d_b[offset],
                                       size / num_operator, bufsize / num_operator);
    }

    // synchronize all the stream operation
    cudaDeviceSynchronize();

    // stop to measure the execution time    
    sdkStopTimer(&timer);

    // print out the result
    int print_idx = 256;
    printf("compared a sample result...\n");
    printf("host: %.6f, device: %.6f\n",  h_a[print_idx] + h_b[print_idx], h_c[print_idx]);

    // Compute and print the performance
    float elapsed_time_msed = sdkGetTimerValue(&timer);
    float bandwidth = 3 * bufsize * sizeof(float) / elapsed_time_msed / 1e6;
    printf("Time= %.3f msec, bandwidth= %f GB/s\n", elapsed_time_msed, bandwidth);

    // delete timer
    sdkDeleteTimer(&timer);

    // terminate operators
    delete [] ls_operator;

    // terminate device memories
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    // terminate host memories
    cudaFreeHost(h_a);
    cudaFreeHost(h_b);
    cudaFreeHost(h_c);
    
    return 0;
}

__global__ void
vecAdd_kernel(float *c, const float* a, const float* b)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    for (int i = 0; i < 500; i++)
        c[idx] = a[idx] + b[idx];
}

void init_buffer(float *data, const int size)
{
    for (int i = 0; i < size; i++) 
        data[i] = rand() / (float)RAND_MAX;
}
```

* ç¨‹åºè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```bash
Priority Range: low(0), high(-1)
stream 0 - elapsed 11.119 ms
stream 3 - elapsed 19.126 ms
stream 1 - elapsed 23.327 ms
stream 2 - elapsed 29.422 ms
compared a sample result...
host: 1.523750, device: 1.523750
Time= 29.730 msec, bandwidth= 27.087332 GB/s
```

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (91).png" alt=""><figcaption></figcaption></figure>

å¯ä»¥çœ‹åˆ°ï¼Œä¼˜å…ˆçº§æœ€é«˜çš„ CUDA æµï¼ˆæµ 21ï¼‰æŠ¢å äº†ç¬¬äºŒä¸ª CUDA æµï¼ˆæœ¬ä¾‹ä¸­ä¸ºæµ 19ï¼‰çš„ä½ç½®ï¼Œå› æ­¤æµ 21 å¯ä»¥åœ¨æµ 19 æ‰§è¡Œå®Œæ¯•å‰å®Œæˆå·¥ä½œã€‚<mark style="color:red;">è¯·æ³¨æ„ï¼Œæ•°æ®ä¼ è¾“é¡ºåºä¸ä¼šå› ä¼˜å…ˆçº§è€Œæ”¹å˜ã€‚</mark>

### ä½¿ç”¨äº‹ä»¶ä¼°è®¡æ ¸å‡½æ•°æ‰§è¡Œçš„æ—¶é—´

ä¹‹å‰çš„ GPU è¿è¡Œæ—¶é—´ä¼°ç®—æœ‰ä¸€ä¸ªå±€é™æ€§ï¼Œé‚£å°±æ˜¯æ— æ³•æµ‹é‡å†…æ ¸çš„æ‰§è¡Œæ—¶é—´ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨ä¸»æœºç«¯ä½¿ç”¨äº†å®šæ—¶ APIã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸»æœºå’Œ GPU åŒæ­¥æ‰èƒ½æµ‹é‡å†…æ ¸æ‰§è¡Œæ—¶é—´ï¼Œè€ƒè™‘åˆ°æ—¶é—´å¼€é”€å’Œå¯¹ç¨‹åºæ€§èƒ½çš„å½±å“ï¼Œè¿™ç§åšæ³•åœ¨å®é™…å·¥ä½œä¸­æ˜¯ä¸ç°å®çš„ã€‚

è¿™å¯ä»¥ä½¿ç”¨ CUDA äº‹ä»¶æ¥è§£å†³ã€‚CUDA äº‹ä»¶ä¸ CUDA æ•°æ®æµä¸€èµ·è®°å½• GPU ç«¯äº‹ä»¶ã€‚CUDA äº‹ä»¶å¯ä»¥æ˜¯åŸºäº GPU çŠ¶æ€çš„äº‹ä»¶ï¼Œå¹¶è®°å½•ç›¸å…³æ—¶é—´ã€‚åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä¼°ç®—å†…æ ¸æ‰§è¡Œæ—¶é—´

CUDA äº‹ä»¶ç”± `cudaEvent_t` å¥æŸ„ç®¡ç†ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `cudaEventCreate()`åˆ›å»ºä¸€ä¸ª CUDA äº‹ä»¶å¥æŸ„ï¼Œå¹¶ä½¿ç”¨ `cudaEventDestroy()` ç»ˆæ­¢å®ƒã€‚è¦è®°å½•äº‹ä»¶æ—¶é—´ï¼Œå¯ä»¥ä½¿ç”¨ `cudaEventRecord()`ã€‚ç„¶åï¼ŒCUDA äº‹ä»¶å¥æŸ„ä¼šè®°å½• GPU çš„äº‹ä»¶æ—¶é—´ã€‚è¯¥å‡½æ•°è¿˜æ¥å— CUDA æµï¼Œè¿™æ ·æˆ‘ä»¬å°±èƒ½æšä¸¾å‡ºç‰¹å®š CUDA æµçš„äº‹ä»¶æ—¶é—´ã€‚è·å¾—å†…æ ¸æ‰§è¡Œçš„å¼€å§‹å’Œç»“æŸäº‹ä»¶åï¼Œå°±å¯ä»¥ä½¿ç”¨<mark style="color:red;">ä»¥æ¯«ç§’ä¸ºå•ä½</mark>çš„ `cudaEventElapsedTime()`æ¥è·å¾—ç»è¿‡æ—¶é—´ã€‚

### ä½¿ç”¨CUDAäº‹ä»¶

ä¿®æ”¹ä¹‹å‰çš„ä»£ç ï¼š

```c
#include <cstdio>
#include <helper_timer.h>

using namespace std;

__global__ void vecAdd_kernel(float *c, const float* a, const float* b);
void init_buffer(float *data, const int size);

int main(int argc, char* argv[])
{
    float *h_a, *h_b, *h_c;
    float *d_a, *d_b, *d_c;
    int size = 1 << 24;
    int bufsize = size * sizeof(float);

    // allocate host memories
    cudaMallocHost((void**)&h_a, bufsize);
    cudaMallocHost((void**)&h_b, bufsize);
    cudaMallocHost((void**)&h_c, bufsize);

    // initialize host values
    srand(2019);
    init_buffer(h_a, size);
    init_buffer(h_b, size);
    init_buffer(h_c, size);

    // allocate device memories
    cudaMalloc((void**)&d_a, bufsize);
    cudaMalloc((void**)&d_b, bufsize);
    cudaMalloc((void**)&d_c, bufsize);

    // copy host -> device
    cudaMemcpyAsync(d_a, h_a, bufsize, cudaMemcpyHostToDevice);
    cudaMemcpyAsync(d_b, h_b, bufsize, cudaMemcpyHostToDevice);

    // initialize the host timer
    StopWatchInterface *timer;
    sdkCreateTimer(&timer);

    cudaEvent_t start, stop;
    // create CUDA events
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    // start to measure the execution time
    sdkStartTimer(&timer);
    cudaEventRecord(start);

    // launch cuda kernel
    dim3 dimBlock(256);
    dim3 dimGrid(size / dimBlock.x);
    vecAdd_kernel<<< dimGrid, dimBlock >>>(d_c, d_a, d_b);

    // record the event right after the kernel execution finished
    cudaEventRecord(stop);

    // Synchronize the device to measure the execution time from the host side
    cudaEventSynchronize(stop); // we also can make synchronization based on CUDA event
    sdkStopTimer(&timer);
    
    // copy device -> host
    cudaMemcpyAsync(h_c, d_c, bufsize, cudaMemcpyDeviceToHost);

    // print out the result
    int print_idx = 256;
    printf("compared a sample result...\n");
    printf("host: %.6f, device: %.6f\n",  h_a[print_idx] + h_b[print_idx], h_c[print_idx]);

    // print estimated kernel execution time
    float elapsed_time_msed = 0.f;
    cudaEventElapsedTime(&elapsed_time_msed, start, stop);
    printf("CUDA event estimated - elapsed %.3f ms \n", elapsed_time_msed);

    // Compute and print the performance
    elapsed_time_msed = sdkGetTimerValue(&timer);
    printf("Host measured time= %.3f msec/s\n", elapsed_time_msed);

    // terminate device memories
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    // terminate host memories
    cudaFreeHost(h_a);
    cudaFreeHost(h_b);
    cudaFreeHost(h_c);

    // delete timer
    sdkDeleteTimer(&timer);

    // terminate CUDA events
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    
    return 0;
}

__global__ void
vecAdd_kernel(float *c, const float* a, const float* b)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    for (int i = 0; i < 500; i++)
        c[idx] = a[idx] + b[idx];
}

void init_buffer(float *data, const int size)
{
    for (int i = 0; i < size; i++) 
        data[i] = rand() / (float)RAND_MAX;
}
```

ä¸Šè¿°ä»£ç ä¸­å®šæ—¶å™¨éœ€è¦ GPU å’Œä¸»æœºåŒæ­¥ï¼Œ ä¸ºäº†åŒæ­¥ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `cudaEventSynchronize(stop)` å‡½æ•°ï¼Œè®©ä¸»æœºçº¿ç¨‹ä¸äº‹ä»¶åŒæ­¥ã€‚&#x20;

åˆ†åˆ«ç”¨ CUDA äº‹ä»¶å’Œè®¡æ—¶å™¨æµ‹é‡æ ¸å‡½æ•°è¿è¡Œæ—¶é—´ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸¤è€…æ˜¯ä¸åŒçš„ã€‚ å¯ä»¥ä½¿ç”¨ NVIDIA Profiler æ¥éªŒè¯ä¸¤ç§æ–¹æ³•è°æ›´å‡†ç¡®ä¸€äº›ï¼Œä½¿ç”¨ `# nvprof ./cuda_event` å‘½ä»¤ï¼Œè¾“å‡ºå¦‚ä¸‹ï¼š

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (92).png" alt=""><figcaption></figcaption></figure>

ç”±ä¸Šå›¾å¯çŸ¥CUDA äº‹ä»¶èƒ½æä¾›å‡†ç¡®çš„ç»“æœã€‚

<mark style="color:red;">ä½¿ç”¨ CUDA äº‹ä»¶çš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¤šä¸ª CUDA æµä¸­åŒæ—¶æµ‹é‡å¤šä¸ªå†…æ ¸çš„æ‰§è¡Œæ—¶é—´ã€‚</mark>

### å¤šæµä¼°è®¡

<mark style="color:red;">`cudaEventRecord()`</mark><mark style="color:red;">å‡½æ•°ä¸ä¸»æœºæ˜¯å¼‚æ­¥çš„ã€‚è¦å®ç°äº‹ä»¶ä¸ä¸»æœºçš„åŒæ­¥ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨</mark> <mark style="color:red;"></mark><mark style="color:red;">`cudaEventSynchronize()`</mark><mark style="color:red;">ã€‚</mark>

```c
#include <cstdio>
#include <helper_timer.h>

using namespace std;

__global__ void vecAdd_kernel(float *c, const float* a, const float* b);
void init_buffer(float *data, const int size);

class Operator
{
private:
    int index;
    StopWatchInterface *p_timer;

    static void CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData);
    void print_time();

    cudaEvent_t start, stop;

protected:
    cudaStream_t stream = nullptr;

public:
    Operator(bool create_stream = true) {
        if (create_stream)
            cudaStreamCreate(&stream);
        sdkCreateTimer(&p_timer);

        // create CUDA events
        cudaEventCreate(&start);
        cudaEventCreate(&stop);
    }

    ~Operator() {
        if (stream != nullptr)
            cudaStreamDestroy(stream);
        sdkDeleteTimer(&p_timer);

        // terminate CUDA events
        cudaEventDestroy(start);
        cudaEventDestroy(stop);
    }

    void set_index(int idx) { index = idx; }
    void async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize);
    void print_kernel_time();

}; // Operator

void Operator::CUDART_CB Callback(cudaStream_t stream, cudaError_t status, void* userData) {
    Operator* this_ = (Operator*) userData;
    this_->print_time();
}

void Operator::print_time() {
    // end timer
    sdkStopTimer(&p_timer);
    float elapsed_time_msed = sdkGetTimerValue(&p_timer);
    printf("stream %2d - elapsed %.3f ms \n", index, elapsed_time_msed);
}

void Operator::print_kernel_time() {
    float elapsed_time_msed = 0.f;
    cudaEventElapsedTime(&elapsed_time_msed, start, stop);
    printf("kernel in stream %2d - elapsed %.3f ms \n", index, elapsed_time_msed);
}

void Operator::async_operation(float *h_c, const float *h_a, const float *h_b,
                          float *d_c, float *d_a, float *d_b,
                          const int size, const int bufsize)
{
    // start timer
    sdkStartTimer(&p_timer);

    // copy host -> device
    cudaMemcpyAsync(d_a, h_a, bufsize, cudaMemcpyHostToDevice, stream);
    cudaMemcpyAsync(d_b, h_b, bufsize, cudaMemcpyHostToDevice, stream);

    // record the event before the kernel execution
    cudaEventRecord(start, stream);

    // launch cuda kernel
    dim3 dimBlock(256);
    dim3 dimGrid(size / dimBlock.x);
    vecAdd_kernel<<< dimGrid, dimBlock, 0, stream >>>(d_c, d_a, d_b);

    // record the event right after the kernel execution finished
    cudaEventRecord(stop, stream);

    // copy device -> host
    cudaMemcpyAsync(h_c, d_c, bufsize, cudaMemcpyDeviceToHost, stream);

    // what happen if we include CUDA event synchronize?
    // QUIZ: cudaEventSynchronize(stop);

    // register callback function
    cudaStreamAddCallback(stream, Operator::Callback, this, 0);
}

class Operator_with_priority: public Operator {
public:
    Operator_with_priority() : Operator(false) {}

    void set_priority(int priority) {
        cudaStreamCreateWithPriority(&stream, cudaStreamNonBlocking, priority);
    }
};

int main(int argc, char* argv[])
{
    float *h_a, *h_b, *h_c;
    float *d_a, *d_b, *d_c;
    int size = 1 << 24;
    int bufsize = size * sizeof(float);
    int num_operator = 4;

    if (argc != 1)
        num_operator = atoi(argv[1]);

    // check the current device supports CUDA stream's prority
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, 0); 
    if (prop.streamPrioritiesSupported == 0) {
        printf("This device does not support priority streams");
        return 1;
    }

    // initialize timer
    StopWatchInterface *timer;
    sdkCreateTimer(&timer);

    // allocate host memories
    cudaMallocHost((void**)&h_a, bufsize);
    cudaMallocHost((void**)&h_b, bufsize);
    cudaMallocHost((void**)&h_c, bufsize);

    // initialize host values
    srand(2019);
    init_buffer(h_a, size);
    init_buffer(h_b, size);
    init_buffer(h_c, size);

    // allocate device memories
    cudaMalloc((void**)&d_a, bufsize);
    cudaMalloc((void**)&d_b, bufsize);
    cudaMalloc((void**)&d_c, bufsize);

    // create list of operation elements
    Operator_with_priority *ls_operator = new Operator_with_priority[num_operator];

    // Get Priority range
    int priority_low, priority_high;
    cudaDeviceGetStreamPriorityRange(&priority_low, &priority_high);
    printf("Priority Range: low(%d), high(%d)\n", priority_low, priority_high);

    // start to measure the execution time
    sdkStartTimer(&timer);
    
    // execute each operator collesponding data
    // priority setting for each CUDA stream
    for (int i = 0; i < num_operator; i++) {
        // int offset = i * size / num_operator;
        ls_operator[i].set_index(i);
        if (i + 1 == num_operator)
            ls_operator[i].set_priority(priority_high);
        else
            ls_operator[i].set_priority(priority_low);
    }

    // operation (copy(H2D), kernel execution, copy(D2H))
    for (int i = 0; i < num_operator; i++) {
        int offset = i * size / num_operator;
        ls_operator[i].async_operation(&h_c[offset], &h_a[offset], &h_b[offset],
                                       &d_c[offset], &d_a[offset], &d_b[offset],
                                       size / num_operator, bufsize / num_operator);
    }

    // synchronize all the stream operation
    cudaDeviceSynchronize();

    // stop to measure the execution time    
    sdkStopTimer(&timer);

    // print each cuda stream execution time
    for (int i = 0; i < num_operator; i++)
        ls_operator[i].print_kernel_time(); 

    // print out the result
    int print_idx = 256;
    printf("compared a sample result...\n");
    printf("host: %.6f, device: %.6f\n",  h_a[print_idx] + h_b[print_idx], h_c[print_idx]);

    // Compute and print the performance
    float elapsed_time_msed = sdkGetTimerValue(&timer);
    float bandwidth = 3 * bufsize * sizeof(float) / elapsed_time_msed / 1e6;
    printf("Time= %.3f msec, bandwidth= %f GB/s\n", elapsed_time_msed, bandwidth);

    // delete timer
    sdkDeleteTimer(&timer);

    // terminate operators
    delete [] ls_operator;

    // terminate device memories
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    // terminate host memories
    cudaFreeHost(h_a);
    cudaFreeHost(h_b);
    cudaFreeHost(h_c);
    
    return 0;
}

__global__ void
vecAdd_kernel(float *c, const float* a, const float* b)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    for (int i = 0; i < 500; i++)
        c[idx] = a[idx] + b[idx];
}

void init_buffer(float *data, const int size)
{
    for (int i = 0; i < size; i++) 
        data[i] = rand() / (float)RAND_MAX;
}
```

* ç¨‹åºè¾“å‡ºç»“æœï¼š

```sh
Priority Range: low(0), high(-1)
stream 0 - elapsed 11.348 ms
stream
3 - elapsed 19.435 ms
stream 1 - elapsed 22.707 ms
stream 2 - elapsed 35.768 ms
kernel in stream 0 - elapsed 6.052 ms
kernel in stream 1 - elapsed 14.820 ms
kernel in stream 2 - elapsed 17.461 ms
kernel in stream 3 - elapsed 6.190 ms
compared a sample result...
host: 1.523750, device: 1.523750
Time= 35.993 msec, bandwidth= 22.373972 GB/s
```

{% hint style="info" %}
å¦‚æœåœ¨ä»£ç ä¸­åŒ…å«äº†`cudaEventSynchronize(stop);`ï¼Œé‚£ä¹ˆåœ¨è¯¥è¯­å¥å¤„ä¼šå‘ç”ŸåŒæ­¥æ“ä½œï¼Œå³ç­‰å¾…ä¸`stop`äº‹ä»¶ç›¸å…³è”çš„CUDAæµä¸­çš„æ‰€æœ‰æ“ä½œå®Œæˆã€‚è¿™æ„å‘³ç€åœ¨ä¸»æœºç«¯çš„ä»£ç å°†ä¼šç­‰å¾…ç›´åˆ°ä¸`stop`äº‹ä»¶ç›¸å…³è”çš„CUDAæµä¸­çš„æ‰€æœ‰æ“ä½œéƒ½å®Œæˆåæ‰ä¼šç»§ç»­æ‰§è¡Œã€‚å› æ­¤ï¼Œåœ¨æ­¤å¤„åŒ…å«`cudaEventSynchronize(stop);`å°†ä¼šå¯¼è‡´ä¸»æœºç«¯çš„ä»£ç åœ¨ç¡®è®¤ä¸CUDAæµç›¸å…³è”çš„æ‰€æœ‰æ“ä½œéƒ½å·²å®Œæˆåæ‰ä¼šç»§ç»­æ‰§è¡Œï¼Œè€Œä¸ä¼šæå‰ç»§ç»­æ‰§è¡Œã€‚
{% endhint %}

### ä½¿ç”¨OpenMPçš„è°ƒåº¦æ“ä½œ

åœ¨ä½¿ç”¨OpenMPçš„åŒæ—¶ä½¿ç”¨CUDAï¼Œä¸ä»…å¯ä»¥æé«˜ä¾¿æºæ€§å’Œç”Ÿäº§æ•ˆç‡ï¼Œè€Œä¸”è¿˜å¯ä»¥æ é«˜ä¸»æœºä»£ç çš„æ€§èƒ½ã€‚åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå¾ªç¯è°ƒåº¦æ“ä½œï¼Œä¸æ­¤ä¸åŒï¼Œ æˆ‘ä»¬ä½¿ç”¨äº†OpenMPçº¿ç¨‹è°ƒåº¦æ“ä½œåˆ°ä¸åŒçš„æµä¸­ï¼Œå…·ä½“æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š

```c
omp_set_num_threads(num_operator);
    #pragma omp parallel
    {
        int i = omp_get_thread_num();
        int offset = i * size / num_operator;
        ls_operator[i].set_index(i);
        ls_operator[i].async_operation(&h_c[offset], &h_a[offset], &h_b[offset],
                                    &d_c[offset], &d_a[offset], &d_b[offset],
                                    size / num_operator, bufsize / num_operator);
    }
```

ç¨‹åºè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```bash
stream 0 - elapsed 10.734 ms
stream 2 - elapsed 16.153 ms
stream 3 - elapsed 21.968 ms
stream 1 - elapsed 27.668 ms
compared a sample result...
host: 1.523750, device: 1.523750
Time= 27.836 msec, bandwidth= 28.930389GB/s
```

æ¯å½“æ‰§è¡Œè¯¥ç¨‹åºæ—¶ï¼Œä½ ä¼šå‘ç°æ¯ä¸ªæ•°æ®æµå®Œæˆå·¥ä½œçš„é¡ºåºéƒ½ä¸ä¸€æ ·ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªæµæ˜¾ç¤ºçš„æ—¶é—´ä¹Ÿä¸åŒã€‚è¿™æ˜¯å› ä¸º OpenMP å¯ä»¥åˆ›å»ºå¤šä¸ªçº¿ç¨‹ï¼Œè€Œæ“ä½œæ˜¯åœ¨è¿è¡Œæ—¶ç¡®å®šçš„ã€‚

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (93).png" alt=""><figcaption></figcaption></figure>
