# ğŸ‘Š Post-Training Quantization

### é‡åŒ–çš„åŸºæœ¬åŸç†: åŸºæœ¬æœ¯è¯­(PTQ, QAT)

æ ¹æ®é‡åŒ–çš„æ—¶æœºï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šæŠŠé‡åŒ–åˆ†ä¸º&#x20;

* PTQ(Post-Training Quantization)ï¼Œè®­ç»ƒåé‡åŒ–&#x20;
* QAT(Quantization-Aware Training)ï¼Œè®­ç»ƒæ—¶é‡åŒ– PTQä¸€èˆ¬æ˜¯æŒ‡å¯¹äºè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œé€šè¿‡calibrationç®—æ³•ç­‰æ¥è·å–dynamic rangeæ¥è¿›è¡Œé‡åŒ–ã€‚ ä½†é‡åŒ–æ™®éä¸Šä¼šäº§ç”Ÿç²¾åº¦ä¸‹é™ã€‚æ‰€ä»¥QATä¸ºäº†å¼¥è¡¥ç²¾åº¦ä¸‹é™ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é€šè¿‡Fine-tuningæƒ é‡æ¥é€‚åº”è¿™ç§è¯¯å·®ï¼Œå®ç°ç²¾åº¦ä¸‹é™çš„æœ€å°åŒ–ã€‚æ‰€ä»¥ä¸€èˆ¬æ¥è®²ï¼ŒQATçš„ç²¾åº¦ä¼šé«˜äºPTQã€‚ä½†å¹¶ä¸ç»å¯¹ã€‚

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (40).png" alt=""><figcaption></figcaption></figure>

### PTQæ˜¯ä»€ä¹ˆ&#x20;

PTQ(Post-training quantization)ä¹Ÿè¢«ç§°ä½œéšå¼é‡åŒ–(implicit quantization)ã€‚æˆ‘ä»¬å¹¶ä¸æ˜¾å¼çš„ å¯¹ç®—å­æ·»åŠ é‡åŒ–èŠ‚ç‚¹(Q/DQ)ï¼Œcalibrationä¹‹åTensorRTæ ¹æ®æƒ…å†µè¿›è¡Œé‡åŒ–

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (41).png" alt="" width="563"><figcaption></figcaption></figure>

trtexecåœ¨é€‰æ‹©å‚æ•°è¿›è¡Œfp16æˆ–è€…int8æŒ‡å®šçš„æ—¶å€™ï¼Œä½¿ç”¨çš„å°±æ˜¯PTQã€‚(int8çš„æ—¶å€™éœ€è¦æŒ‡å®š calibration dataset)ã€‚å¾ˆæ–¹ä¾¿ä½¿ç”¨ï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦å…ˆç†è§£PTQçš„åˆ©å¼Šã€‚

å…·ä½“ä½¿ç”¨å°±æ˜¯ï¼Œæˆ‘ä»¬**å¯¼å‡ºONNXæ¨¡å‹**ï¼Œè½¬æ¢ä¸ºTensorRTçš„è¿‡ç¨‹ä¸­å¯ä»¥ä½¿ç”¨trtæä¾›çš„Calibrationæ–¹æ³•å»æ ¡å‡†ï¼Œè¿™ä¸ªä½¿ç”¨èµ·æ¥æ¯”è¾ƒç®€å•ã€‚å¯ä»¥ç›´æ¥ä½¿ç”¨trtå®˜æ–¹æä¾›çš„`trtexec`å‘½ä»¤å»å®ç°ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨trtæä¾›çš„pythonæˆ–è€…C++çš„APIæ¥å£å»é‡åŒ–ï¼Œæ¯”è¾ƒå®¹æ˜“ã€‚

ç›®å‰ï¼ŒTensorRTæä¾›çš„åè®­ç»ƒé‡åŒ–ç®—æ³•ä¹Ÿå¤šäº†å¥½å¤šï¼Œåˆ†åˆ«é€‚åˆäºä¸åŒçš„ä»»åŠ¡ï¼š

* EntropyCalibratorV2

> Entropy calibration chooses the tensorâ€™s scale factor to optimize the quantized tensorâ€™s information-theoretic content, and usually suppresses outliers in the distribution. This is the current and recommended entropy calibrator and is required for DLA. Calibration happens before Layer fusion by default. <mark style="color:red;">It is recommended for CNN-based networks.</mark>

* MinMaxCalibrator

> This calibrator uses the entire range of the activation distribution to determine the scale factor. It seems to work better for NLP tasks. Calibration happens before Layer fusion by default. <mark style="color:red;">This is recommended for networks such as NVIDIA BERT (an optimized version of Google's official implementation).</mark>

* EntropyCalibrator

> This is the original entropy calibrator. It is less complicated to use than the LegacyCalibrator and typically produces better results. <mark style="color:red;">Calibration happens after Layer fusion by default.</mark>

* LegacyCalibrator

> This calibrator is for compatibility with TensorRT 2.0 EA. This calibrator requires user parameterization and is provided as a fallback option if the other calibrators yield poor results. Calibration happens after Layer fusion by default. You can customize this calibrator to implement percentile max, for example, 99.99% percentile max is observed to have best accuracy for NVIDIA BERT.

é€šè¿‡ä¸Šè¿°è¿™äº›ç®—æ³•é‡åŒ–æ—¶ï¼ŒTensorRTä¼šåœ¨ä¼˜åŒ–ç½‘ç»œçš„æ—¶å€™å°è¯•INT8ç²¾åº¦ï¼Œå‡å¦‚æŸä¸€å±‚åœ¨INT8ç²¾åº¦ä¸‹é€Ÿåº¦ä¼˜äºé»˜è®¤ç²¾åº¦ï¼ˆFP32æˆ–è€…FP16ï¼‰åˆ™ä¼˜å…ˆä½¿ç”¨`INT8`ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬**æ— æ³•æ§åˆ¶æŸä¸€å±‚çš„ç²¾åº¦**ï¼Œå› ä¸ºTensorRTæ˜¯ä»¥é€Ÿåº¦ä¼˜åŒ–ä¸ºä¼˜å…ˆçš„ï¼ˆå¾ˆæœ‰å¯èƒ½æŸä¸€å±‚ä½ æƒ³è®©å®ƒè·‘int8ç»“æœå´æ˜¯fp32ï¼‰ã€‚å³ä½¿æˆ‘ä»¬ä½¿ç”¨APIå»è®¾ç½®ä¹Ÿä¸è¡Œï¼Œæ¯”å¦‚`set_precision`è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºTensorRTè¿˜ä¼šåšå›¾çº§åˆ«çš„ä¼˜åŒ–ï¼Œå®ƒå¦‚æœå‘ç°è¿™ä¸ª`op`ï¼ˆæ˜¾å¼è®¾ç½®äº†`INT8`ç²¾åº¦ï¼‰å’Œå¦ä¸€ä¸ª`op`å¯ä»¥åˆå¹¶ï¼Œå°±ä¼šå¿½ç•¥ä½ è®¾ç½®çš„`INT8`ç²¾åº¦ã€‚

è¯´ç™½äº†å°±æ˜¯ä¸å¥½æ§åˆ¶ã€‚æˆ‘ä¹Ÿå°è¯•è¿‡è¿™ç§æ–¹å¼ï¼Œç®€å•æƒ…å†µï¼Œç®€å•æ¨¡å‹é—®é¢˜ä¸å¤§ï¼ˆresnetç³»åˆ—ï¼‰ï¼Œæ¶‰åŠåˆ°æ¯”è¾ƒå¤æ‚çš„ï¼ˆtransformerï¼‰è¿™ä¸ªè®¾ç½®ç²¾åº¦å¯èƒ½ä¸ç®¡ç”¨ã€‚

### PTQ(ä¼˜ç¼ºç‚¹åˆ†æ)&#x20;

**ä¼˜ç‚¹ ï¼š** æ–¹ä¾¿ä½¿ç”¨ï¼Œä¸éœ€è¦è®­ç»ƒã€‚å¯ä»¥åœ¨éƒ¨ç½²è®¾å¤‡ä¸Šç›´æ¥è·‘&#x20;

**ç¼ºç‚¹ï¼š**

1. **ç²¾åº¦ä¸‹é™ï¼š**é‡åŒ–è¿‡ç¨‹ä¼šå¯¼è‡´ç²¾åº¦ä¸‹é™ã€‚ä½†PTQæ²¡æœ‰ç±»ä¼¼äºQATè¿™ç§fine-tuningçš„è¿‡ç¨‹ã€‚æ‰€ä»¥æƒé‡ä¸ä¼šæ›´æ–°æ¥å¸æ”¶è¿™ç§è¯¯å·®
2. **é‡åŒ–ä¸å¯æ§ï¼š**TensorRTä¼šæƒè¡¡é‡åŒ–åæ‰€äº§ç”Ÿçš„æ–°æ·»çš„è®¡ç®—æˆ–è€…è®¿å­˜ï¼Œ æ˜¯å¦ç”¨INT8è¿˜æ˜¯FP16ã€‚TensorRTä¸­çš„kernel autotuningä¼šé€‰æ‹©æ ¸å‡½æ•°æ¥åšFP16/INT8çš„è®¡ç®—ã€‚æ¥æŸ¥çœ‹æ˜¯å¦åœ¨CUDA coreä¸Šè·‘è¿˜æ˜¯åœ¨Tensor coreä¸Šè·‘ï¼Œæœ‰å¯èƒ½FP16æ˜¯åœ¨Tensor coreä¸Šï¼Œä½†è½¬ä¸ºINT8ä¹‹åå°±åœ¨CUDA coreä¸Šäº†
3. **å±‚èåˆé—®é¢˜ï¼š**é‡åŒ–åæœ‰å¯èƒ½å‡ºç°ä¹‹å‰å¯ä»¥èåˆçš„å±‚ï¼Œä¸èƒ½èåˆäº† ï¼Œé‡åŒ–ä¼šæ·»åŠ reformatterè¿™ç§æ›´æ”¹tensorçš„æ ¼å¼çš„ç®—å­ï¼Œå¦‚æœæœ¬æ¥èåˆçš„ä¸¤ä¸ªç®—å­é—´æ·»åŠ äº†è¿™ ä¸ªå°±ä¸èƒ½è¢«èåˆäº†ï¼Œ æ¯”å¦‚æœ‰äº›ç®—å­æ”¯æŒint8ï¼Œä½†æŸäº›ä¸æ”¯æŒã€‚ä¹‹å‰å¯ä»¥èåˆçš„ï¼Œä½†å› ä¸ºç²¾åº¦ä¸åŒä¸èƒ½èåˆäº† å¦‚æœINT8é‡åŒ–åé€Ÿåº¦åè€Œä¼šæ¯”FP16/FP32è¦æ…¢ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸Šçš„2å’Œ3å»åˆ†æå¹¶æ’æŸ¥åŸå› 

### é‡åŒ–ä¸­çš„sensitive analysis&#x20;

ä»ç²¾åº¦åˆ†æçš„è§’åº¦å»å¼¥è¡¥PTQçš„ç²¾åº¦ä¸‹é™ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œ`layer-wise`çš„é‡åŒ–åˆ†æã€‚è¿™ç§æ–¹æ³•è¢«ç§° ä½œl`ayer-wise sensitive analysis`

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (43).png" alt=""><figcaption></figcaption></figure>

æ™®éæ¥è®²ï¼Œæ¨¡å‹æ¡†æ¶ä¸­ä¼šæœ‰ä¸€äº›å±‚çš„é‡åŒ–å¯¹ç²¾åº¦çš„å½±å“æ¯”è¾ƒå¤§ã€‚æˆ‘ä»¬ç®¡å®ƒä»¬å«åšæ•æ„Ÿå±‚(sensitive layer)ã€‚å¯¹äºè¿™äº›æ•æ„Ÿå±‚çš„é‡åŒ–æˆ‘ä»¬éœ€è¦éå¸¸å°å¿ƒï¼Œå°½é‡ç”¨FP16ã€‚<mark style="color:red;">æ•æ„Ÿå±‚ä¸€èˆ¬é è¿‘æ¨¡å‹çš„è¾“å…¥è¾“å‡ºã€‚</mark>

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (44).png" alt=""><figcaption></figcaption></figure>

### å­¦ä¹ ä½¿ç”¨Polygraphy&#x20;

å¯¹äºè¿™ç§`sensitive analysis`ï¼Œå…¶å®NVIDIAæœ€è¿‘å‡ å¹´æ¨å‡ºäº†ä¸€ä¸ªå«åš`polygraphy`çš„ä¸€ä¸ªå·¥å…·ã€‚å¯ä»¥ç”¨æ¥ åˆ†æå¹¶æŸ¥æ‰¾æ¨¡å‹ç²¾åº¦ä¸‹é™å¹¶ä¸”å½±å“æ¯”è¾ƒå¤§çš„åœ°æ–¹ã€‚éå¸¸å¥½çš„å·¥å…·ï¼Œåš`TensorRT`é‡åŒ–å¿…é¡»è¦æŒæ¡ã€‚èƒ½åš çš„äº‹æƒ…å¤ªå¤šï¼Œæ¯”å¦‚ï¼š

* `onnxruntime`ä¸TensorRT engineçš„`layer-wise`çš„ç²¾åº¦åˆ†æ
* è¾“å‡ºæ¯ä¸€å±‚`layer`çš„æƒé‡`histogram`&#x20;
* æˆªå–å½±å“æ•´ä¸ªç½‘ç»œä¸­å¯¹ç²¾åº¦å½±å“æœ€å¤§çš„å­ç½‘ï¼Œå¹¶ä½¿ç”¨`onnx-surgeon`å•ç‹¬æ‹¿å‡ºæ¥

### FP16/INT8å¯¹è®¡ç®—èµ„æºçš„åˆ©ç”¨

æˆ‘ä»¬åœ¨åšé‡åŒ–åï¼Œæˆ‘ä»¬æ— æ³•æŒ‡å®šå°†é‡åŒ–åçš„convæˆ–è€…gemmæ”¾åœ¨Tensor coreè¿˜æ˜¯åœ¨CUDA coreä¸Šè®¡ç®—ã€‚ è¿™äº›æ˜¯TensorRTåœ¨å¸®æˆ‘ä»¬é€‰æ‹©æ ¸å‡½æ•°çš„æ—¶å€™è‡ªåŠ¨å®Œæˆçš„ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•æŸ¥çœ‹æˆ‘ä»¬æ˜¯å¦åœ¨ç”¨Tensor coreå‘¢ï¼Ÿ æˆ‘ä»¬ä¸€èˆ¬æœ‰è¿™ä¹ˆå‡ ä¸ªåŠæ³•&#x20;

* ä½¿ç”¨`dlprof`&#x20;
* ä½¿ç”¨`nsight system`
* ä½¿ç”¨`trtexec`

### DLProf&#x20;

DLProf (Deep learning Profiler)å·¥å…·å¯ä»¥æŠŠæ¨¡å‹åœ¨GPUä¸Šçš„æ‰§è¡Œæƒ…å†µä»¥TensorBoardçš„å½¢å¼æ‰“å°å‡ºæ¥ï¼Œ åˆ†æTensorCoreçš„ä½¿ç”¨æƒ…å†µã€‚æ„Ÿå…´è¶£çš„å¯ä»¥æŸ¥çœ‹ä¸€ä¸‹ã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒDLProfä¸æ”¯æŒJetsonç³»åˆ—çš„ Profileã€‚å¯¹äºJetsonï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Nsight systemæˆ–è€…trtexec

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (45).png" alt="" width="563"><figcaption></figcaption></figure>

### Nsight System/trtexec&#x20;

å¦‚æœæ˜¯åˆ©ç”¨Nsight systemçš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹åˆ°å“ªä¸€ä¸ªkernelçš„æ—¶é—´å ç”¨ç‡æœ€é«˜ï¼Œä¹‹åä»kernelçš„åå­— å–æ¨æµ‹è¿™ä¸ªkernelæ˜¯å¦åœ¨ç”¨Tensor Coreã€‚(ä»kernelåå­—æ¨æµ‹kernelçš„è®¡ç®—è®¾å¤‡éœ€è¦ç»éªŒ)

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (46).png" alt=""><figcaption></figcaption></figure>

ä»kernelåå­—æ¨æµ‹å¯ä»¥ä»kernelä¸­çš„å…³é”®å­—å»çŒœï¼Œæ¯”å¦‚&#x20;

* &#x20;h884 = HMMA = FP16 TensorCore&#x20;
* i8816 = IMMA = INT8 TensorCore&#x20;
* &#x20;hcudnn = FP16 normal CUDA kernel (`without TensorCore)`&#x20;
* &#x20;icudnn = INT8 normal CUDA kernel (`without TensorCore`)&#x20;
* scudnn = FP32 normal CUDA kernel (`without TensorCore`)

> `HMMA: Half-precision matrix multiply and accumulate`&#x20;
>
> `IMMA: Int-precision matrix multiply and accumulate`&#x20;

### reference

* [https://mp.weixin.qq.com/s/DlMC7H4Wh8CaDUEEHL3yNQ](https://mp.weixin.qq.com/s/DlMC7H4Wh8CaDUEEHL3yNQ)





