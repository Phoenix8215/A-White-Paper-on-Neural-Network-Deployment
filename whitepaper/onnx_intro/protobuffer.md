# ğŸ˜‰ ONNXä¸­çš„å„ç±»Proto

### **ä¸­é—´è¡¨ç¤º â€”â€” ONNX**

åœ¨ä»‹ç» ONNX ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»æœ¬è´¨ä¸Šæ¥è®¤è¯†ä¸€ä¸‹ç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚ç¥ç»ç½‘ç»œå®é™…ä¸Šåªæ˜¯æè¿°äº†æ•°æ®è®¡ç®—çš„è¿‡ç¨‹ï¼Œå…¶ç»“æ„å¯ä»¥ç”¨è®¡ç®—å›¾è¡¨ç¤ºã€‚æ¯”å¦‚ a+b å¯ä»¥ç”¨ä¸‹é¢çš„è®¡ç®—å›¾æ¥è¡¨ç¤ºï¼š

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (2) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

<mark style="color:red;">ä¸ºäº†åŠ é€Ÿè®¡ç®—ï¼Œä¸€äº›æ¡†æ¶ä¼šä½¿ç”¨å¯¹ç¥ç»ç½‘ç»œâ€œå…ˆç¼–è¯‘ï¼Œåæ‰§è¡Œâ€çš„é™æ€å›¾æ¥æè¿°ç½‘ç»œã€‚é™æ€å›¾çš„ç¼ºç‚¹æ˜¯éš¾ä»¥æè¿°æ§åˆ¶æµï¼ˆæ¯”å¦‚ if-else åˆ†æ”¯è¯­å¥å’Œ for å¾ªç¯è¯­å¥ï¼‰</mark>ï¼Œç›´æ¥å¯¹å…¶å¼•å…¥æ§åˆ¶è¯­å¥ä¼šå¯¼è‡´äº§ç”Ÿä¸åŒçš„è®¡ç®—å›¾ã€‚æ¯”å¦‚å¾ªç¯æ‰§è¡Œ n æ¬¡ a=a+bï¼Œå¯¹äºä¸åŒçš„ nï¼Œä¼šç”Ÿæˆä¸åŒçš„è®¡ç®—å›¾ï¼š

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (3) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

ONNX ï¼ˆOpen Neural Network Exchangeï¼‰æ˜¯ Facebook å’Œå¾®è½¯åœ¨2017å¹´å…±åŒå‘å¸ƒçš„ï¼Œç”¨äºæ ‡å‡†æè¿°è®¡ç®—å›¾çš„ä¸€ç§æ ¼å¼ã€‚ç›®å‰ï¼Œåœ¨æ•°å®¶æœºæ„çš„å…±åŒç»´æŠ¤ä¸‹ï¼ŒONNX å·²ç»å¯¹æ¥äº†å¤šç§æ·±åº¦å­¦ä¹ æ¡†æ¶å’Œå¤šç§æ¨ç†å¼•æ“ã€‚å› æ­¤ï¼ŒONNX è¢«å½“æˆäº†æ·±åº¦å­¦ä¹ æ¡†æ¶åˆ°æ¨ç†å¼•æ“çš„æ¡¥æ¢ï¼Œå°±åƒç¼–è¯‘å™¨çš„ä¸­é—´è¯­è¨€ä¸€æ ·ã€‚<mark style="color:red;">ç”±äºå„æ¡†æ¶å…¼å®¹æ€§ä¸ä¸€ï¼Œæˆ‘ä»¬é€šå¸¸åªç”¨ ONNX è¡¨ç¤ºæ›´å®¹æ˜“éƒ¨ç½²çš„é™æ€å›¾ã€‚</mark>

### å„ç±»Proto

ONNXæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œçš„æ ¼å¼ï¼Œé‡‡ç”¨`Protobuf`äºŒè¿›åˆ¶å½¢å¼è¿›è¡Œåºåˆ—åŒ–æ¨¡å‹ã€‚ `Protobuf`ä¼šæ ¹æ®ç”¨äºå®šä¹‰çš„æ•°æ®ç»“æ„æ¥è¿›è¡Œåºåˆ—åŒ–å­˜å‚¨ åŒç†ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å®˜æ–¹æä¾›çš„æ•°æ®ç»“æ„ä¿¡æ¯ï¼Œå»ä¿®æ”¹æˆ–è€…åˆ›å»ºonnxã€‚

<figure><img src="../../.gitbook/assets/image-20240407112310900.png" alt=""><figcaption></figcaption></figure>

onnxçš„å„ç±»protoçš„å®šä¹‰éœ€è¦çœ‹å®˜æ–¹æ–‡æ¡£([https://github.com/onnx/onnx/tree/main](https://github.com/onnx/onnx/tree/main)) ã€‚è¿™é‡Œé¢çš„`onnx/onnx.in.proto`å®šä¹‰äº†æ‰€æœ‰onnxçš„Protoã€‚æœ‰å…³onnxçš„IR(`intermediate representation`)ä¿¡æ¯ï¼Œçœ‹è¿™é‡Œ([https://github.com/onnx/onnx/blob/main/docs/IR.md](https://github.com/onnx/onnx/blob/main/docs/IR.md))

### ç†è§£onnxä¸­çš„ç»„ç»‡ç»“æ„

* `ModelProto`(æè¿°çš„æ˜¯æ•´ä¸ªæ¨¡å‹çš„ä¿¡æ¯)
* `GraphProto`(æè¿°çš„æ˜¯æ•´ä¸ªç½‘ç»œçš„ä¿¡æ¯)
* `NodeProto` (æè¿°çš„æ˜¯å„ä¸ªè®¡ç®—èŠ‚ç‚¹ï¼Œæ¯”å¦‚conv, linear)
* `TensorProto` (æè¿°çš„æ˜¯tensorçš„ä¿¡æ¯ï¼Œä¸»è¦åŒ…æ‹¬æƒé‡)
* `ValueInfoProto` (æè¿°çš„æ˜¯input/outputä¿¡æ¯)
* `AttributeProto` (æè¿°çš„æ˜¯nodeèŠ‚ç‚¹çš„å„ç§å±æ€§ä¿¡æ¯)

#### onnxä¸­çš„ValueInfoProto

ä¸€èˆ¬ç”¨æ¥å®šä¹‰ç½‘ç»œçš„`input/output` ï¼Œä¼šæ ¹æ®`input/output`çš„`type`æ¥é™„åŠ å±æ€§

<figure><img src="../../.gitbook/assets/image-20240407122349880-1.png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image-20240407122445898.png" alt="" width="563"><figcaption></figcaption></figure>

#### onnxä¸­çš„TensorProto

<figure><img src="../../.gitbook/assets/image-20240407122743299.png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image-20240407122853847.png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image-20240407122924505.png" alt="" width="563"><figcaption></figcaption></figure>

ä¸€èˆ¬ç”¨æ¥å®šä¹‰ä¸€ä¸ªæƒé‡ï¼Œæ¯”å¦‚`conv`çš„`w`å’Œ`b` ï¼Œ`dims`æ˜¯`repeated`ç±»å‹ï¼Œæ„å‘³ç€æ˜¯æ•°ç»„ï¼Œ`raw_data`æ˜¯`bytes`ç±»å‹ã€‚

#### onnxä¸­çš„NodeProto

<figure><img src="../../.gitbook/assets/image-20240407125750798.png" alt=""><figcaption></figcaption></figure>

ä¸€èˆ¬ç”¨æ¥å®šä¹‰ä¸€ä¸ªè®¡ç®—èŠ‚ç‚¹ï¼Œæ¯”å¦‚`conv`, `linear` ï¼Œ`input`æ˜¯`repeated`ç±»å‹ï¼Œæ„å‘³ç€æ˜¯æ•°ç»„ï¼Œ`output`æ˜¯`repeated`ç±»å‹ï¼Œæ„å‘³ç€æ˜¯æ•°ç»„ï¼Œ`attribute`æœ‰ä¸€ä¸ªè‡ªå·±çš„`Proto`ï¼Œ`op_type`éœ€è¦ä¸¥æ ¼æ ¹æ®`onnx`æ‰€æä¾›çš„`Operators`å†™ã€‚

#### onnxä¸­çš„AttributeProto

<figure><img src="../../.gitbook/assets/image-20240407130648541.png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image-20240407130708914.png" alt="" width="563"><figcaption></figcaption></figure>

ä¸€èˆ¬ç”¨æ¥å®šä¹‰ä¸€ä¸ª`node`çš„å±æ€§ã€‚æ¯”å¦‚è¯´`kernel size`ï¼Œæ¯”è¾ƒå¸¸è§çš„æ–¹å¼å°±æ˜¯æŠŠ`(key, value)`ä¼ å…¥`Proto`ï¼Œä¹‹å `name = key ï¼Œi = value` ã€‚

#### onnxä¸­çš„GraphProto

<figure><img src="../../.gitbook/assets/image-20240407131258053.png" alt="" width="563"><figcaption></figcaption></figure>

ä¸€èˆ¬ç”¨æ¥å®šä¹‰æ¨¡å‹çš„å…¨å±€ä¿¡æ¯ï¼Œæ¯”å¦‚`opset` ï¼Œ`graph`å¹¶ä¸æ˜¯`repeated`ï¼Œæ‰€ä»¥ä¸€ä¸ª`model`å¯¹åº”ä¸€ä¸ª`graph`ã€‚

`ONNX`æä¾›äº†ä¸€äº›å¾ˆæ–¹ä¾¿çš„apiæ¥åˆ›å»º`ONNX`ï¼š`onnx.helper.make_tensor` `onnx.helper.make_tensor_value_info` ï¼Œ`onnx.helper.make_attribute` `onnx.helper.make_node` ï¼Œ`onnx.helper.make_graph` ï¼Œ`onnx.helper.make_model`ã€‚

### ä½¿ç”¨APIæ„å»ºONNXæ¨¡å‹(<mark style="color:red;">from scratch</mark>)

* example 1

```python
import onnx
from onnx import helper
from onnx import TensorProto

# ç†è§£onnxä¸­çš„ç»„ç»‡ç»“æ„
#   - ModelProto (æè¿°çš„æ˜¯æ•´ä¸ªæ¨¡å‹çš„ä¿¡æ¯)
#   --- GraphProto (æè¿°çš„æ˜¯æ•´ä¸ªç½‘ç»œçš„ä¿¡æ¯)
#   ------ NodeProto (æè¿°çš„æ˜¯å„ä¸ªè®¡ç®—èŠ‚ç‚¹ï¼Œæ¯”å¦‚conv, linear)
#   ------ TensorProto (æè¿°çš„æ˜¯tensorçš„ä¿¡æ¯ï¼Œä¸»è¦åŒ…æ‹¬æƒé‡)
#   ------ ValueInfoProto (æè¿°çš„æ˜¯input/outputä¿¡æ¯)
#   ------ AttributeProto (æè¿°çš„æ˜¯nodeèŠ‚ç‚¹çš„å„ç§å±æ€§ä¿¡æ¯)


def create_onnx():
    # åˆ›å»ºValueProto
    a = helper.make_tensor_value_info('a', TensorProto.FLOAT, [10, 10])
    x = helper.make_tensor_value_info('x', TensorProto.FLOAT, [10, 10])
    b = helper.make_tensor_value_info('b', TensorProto.FLOAT, [10, 10])
    y = helper.make_tensor_value_info('y', TensorProto.FLOAT, [10, 10])

    # åˆ›å»ºNodeProto
    # åªèƒ½å¡«å†™onnxæ”¯æŒçš„ç®—å­åç§°ï¼Œä¸èƒ½çå†™
    mul = helper.make_node('Mul', ['a', 'x'], 'c', "multiply")
    add = helper.make_node('Add', ['c', 'b'], 'y', "add")

    # æ„å»ºGraphProto
    graph = helper.make_graph([mul, add], 'sample-linear', [a, x, b], [y])

    # æ„å»ºModelProto
    model = helper.make_model(graph)

    # æ£€æŸ¥modelæ˜¯å¦æœ‰é”™è¯¯
    onnx.checker.check_model(model)
    # print(model)

    # ä¿å­˜model
    onnx.save(model, "sample-linear.onnx")

    return model


if __name__ == "__main__":
    model = create_onnx()

```

* example 2

```python
import numpy as np
import onnx
from onnx import numpy_helper


def create_initializer_tensor(
        name: str,
        tensor_array: np.ndarray,
        data_type: onnx.TensorProto = onnx.TensorProto.FLOAT
) -> onnx.TensorProto:

    initializer = onnx.helper.make_tensor(
        name      = name,
        data_type = data_type,
        dims      = tensor_array.shape,
        vals      = tensor_array.flatten().tolist())

    return initializer


def main():
    
    input_batch    = 1
    input_channel  = 3
    input_height   = 64
    input_width    = 64
    output_channel = 16

    input_shape    = [input_batch, input_channel, input_height, input_width]
    output_shape   = [input_batch, output_channel, 1, 1]

    ##########################åˆ›å»ºinput/output################################
    model_input_name  = "input0"
    model_output_name = "output0"

    input = onnx.helper.make_tensor_value_info(
            model_input_name,
            onnx.TensorProto.FLOAT,
            input_shape)

    output = onnx.helper.make_tensor_value_info(
            model_output_name, 
            onnx.TensorProto.FLOAT, 
            output_shape)
    

    ##########################åˆ›å»ºç¬¬ä¸€ä¸ªconvèŠ‚ç‚¹##############################
    conv1_output_name = "conv2d_1.output"
    conv1_in_ch       = input_channel
    conv1_out_ch      = 32
    conv1_kernel      = 3
    conv1_pads        = 1

    # åˆ›å»ºconvèŠ‚ç‚¹çš„æƒé‡ä¿¡æ¯
    conv1_weight    = np.random.rand(conv1_out_ch, conv1_in_ch, conv1_kernel, conv1_kernel)
    conv1_bias      = np.random.rand(conv1_out_ch)

    conv1_weight_name = "conv2d_1.weight"
    conv1_weight_initializer = create_initializer_tensor(
        name         = conv1_weight_name,
        tensor_array = conv1_weight,
        data_type    = onnx.TensorProto.FLOAT)


    conv1_bias_name  = "conv2d_1.bias"
    conv1_bias_initializer = create_initializer_tensor(
        name         = conv1_bias_name,
        tensor_array = conv1_bias,
        data_type    = onnx.TensorProto.FLOAT)

    # åˆ›å»ºconvèŠ‚ç‚¹ï¼Œæ³¨æ„convèŠ‚ç‚¹çš„è¾“å…¥æœ‰3ä¸ª: input, w, b
    conv1_node = onnx.helper.make_node(
        name         = "conv2d_1",
        op_type      = "Conv",
        inputs       = [
            model_input_name, 
            conv1_weight_name,
            conv1_bias_name
        ],
        outputs      = [conv1_output_name],
        kernel_shape = [conv1_kernel, conv1_kernel],
        pads         = [conv1_pads, conv1_pads, conv1_pads, conv1_pads],
    )

    ##########################åˆ›å»ºä¸€ä¸ªBatchNormèŠ‚ç‚¹###########################
    bn1_output_name = "batchNorm1.output"

    # ä¸ºBNèŠ‚ç‚¹æ·»åŠ æƒé‡ä¿¡æ¯
    bn1_scale = np.random.rand(conv1_out_ch)
    bn1_bias  = np.random.rand(conv1_out_ch)
    bn1_mean  = np.random.rand(conv1_out_ch)
    bn1_var   = np.random.rand(conv1_out_ch)

    # é€šè¿‡create_initializer_tensoråˆ›å»ºæƒé‡ï¼Œæ–¹æ³•å’Œåˆ›å»ºconvèŠ‚ç‚¹ä¸€æ ·
    bn1_scale_name = "batchNorm1.scale"
    bn1_bias_name  = "batchNorm1.bias"
    bn1_mean_name  = "batchNorm1.mean"
    bn1_var_name   = "batchNorm1.var"

    bn1_scale_initializer = create_initializer_tensor(
        name         = bn1_scale_name,
        tensor_array = bn1_scale,
        data_type    = onnx.TensorProto.FLOAT)
    bn1_bias_initializer = create_initializer_tensor(
        name         = bn1_bias_name,
        tensor_array = bn1_bias,
        data_type    = onnx.TensorProto.FLOAT)
    bn1_mean_initializer = create_initializer_tensor(
        name         = bn1_mean_name,
        tensor_array = bn1_mean,
        data_type    = onnx.TensorProto.FLOAT)
    bn1_var_initializer  = create_initializer_tensor(
        name         = bn1_var_name,
        tensor_array = bn1_var,
        data_type    = onnx.TensorProto.FLOAT)

    # åˆ›å»ºBNèŠ‚ç‚¹ï¼Œæ³¨æ„BNèŠ‚ç‚¹çš„è¾“å…¥ä¿¡æ¯æœ‰5ä¸ª: input, scale, bias, mean, var
    bn1_node = onnx.helper.make_node(
        name    = "batchNorm1",
        op_type = "BatchNormalization",
        inputs  = [
            conv1_output_name,
            bn1_scale_name,
            bn1_bias_name,
            bn1_mean_name,
            bn1_var_name
        ],
        outputs=[bn1_output_name],
    )

    ##########################åˆ›å»ºä¸€ä¸ªReLUèŠ‚ç‚¹###########################
    relu1_output_name = "relu1.output"

    # åˆ›å»ºReLUèŠ‚ç‚¹ï¼ŒReLUä¸éœ€è¦æƒé‡ï¼Œæ‰€ä»¥ç›´æ¥make_nodeå°±å¥½äº†
    relu1_node = onnx.helper.make_node(
        name    = "relu1",
        op_type = "Relu",
        inputs  = [bn1_output_name],
        outputs = [relu1_output_name],
    )

    ##########################åˆ›å»ºä¸€ä¸ªAveragePoolèŠ‚ç‚¹####################
    avg_pool1_output_name = "avg_pool1.output"

    # åˆ›å»ºAvgPoolèŠ‚ç‚¹ï¼ŒAvgPoolä¸éœ€è¦æƒé‡ï¼Œæ‰€ä»¥ç›´æ¥make_nodeå°±å¥½äº†
    avg_pool1_node = onnx.helper.make_node(
        name    = "avg_pool1",
        op_type = "GlobalAveragePool",
        inputs  = [relu1_output_name],
        outputs = [avg_pool1_output_name],
    )

    ##########################åˆ›å»ºç¬¬äºŒä¸ªconvèŠ‚ç‚¹##############################

    # åˆ›å»ºconvèŠ‚ç‚¹çš„å±æ€§
    conv2_in_ch  = conv1_out_ch
    conv2_out_ch = output_channel
    conv2_kernel = 1
    conv2_pads   = 0

    # åˆ›å»ºconvèŠ‚ç‚¹çš„æƒé‡ä¿¡æ¯
    conv2_weight    = np.random.rand(conv2_out_ch, conv2_in_ch, conv2_kernel, conv2_kernel)
    conv2_bias      = np.random.rand(conv2_out_ch)
    
    conv2_weight_name = "conv2d_2.weight"
    conv2_weight_initializer = create_initializer_tensor(
        name         = conv2_weight_name,
        tensor_array = conv2_weight,
        data_type    = onnx.TensorProto.FLOAT)

    conv2_bias_name  = "conv2d_2.bias"
    conv2_bias_initializer = create_initializer_tensor(
        name         = conv2_bias_name,
        tensor_array = conv2_bias,
        data_type    = onnx.TensorProto.FLOAT)

    # åˆ›å»ºconvèŠ‚ç‚¹ï¼Œæ³¨æ„convèŠ‚ç‚¹çš„è¾“å…¥æœ‰3ä¸ª: input, w, b
    conv2_node = onnx.helper.make_node(
        name         = "conv2d_2",
        op_type      = "Conv",
        inputs       = [
            avg_pool1_output_name,
            conv2_weight_name,
            conv2_bias_name
        ],
        outputs      = [model_output_name],
        kernel_shape = [conv2_kernel, conv2_kernel],
        pads         = [conv2_pads, conv2_pads, conv2_pads, conv2_pads],
    )

    ##########################åˆ›å»ºgraph##############################
    graph = onnx.helper.make_graph(
        name    = "sample-convnet",
        inputs  = [input],
        outputs = [output],
        nodes   = [
            conv1_node, 
            bn1_node, 
            relu1_node, 
            avg_pool1_node, 
            conv2_node],
        initializer =[
            conv1_weight_initializer, 
            conv1_bias_initializer,
            bn1_scale_initializer, 
            bn1_bias_initializer,
            bn1_mean_initializer, 
            bn1_var_initializer,
            conv2_weight_initializer, 
            conv2_bias_initializer
        ],
    )

    ##########################åˆ›å»ºmodel##############################
    model = onnx.helper.make_model(graph, producer_name="onnx-sample")
    model.opset_import[0].version = 12
    
    ##########################éªŒè¯&ä¿å­˜model##############################
    model = onnx.shape_inference.infer_shapes(model)
    onnx.checker.check_model(model)
    print("Congratulations!! Succeed in creating {}.onnx".format(graph.name))
    onnx.save(model, "sample-convnet.onnx")


# ä½¿ç”¨onnx.helperåˆ›å»ºä¸€ä¸ªæœ€åŸºæœ¬çš„ConvNet
#         input (ch=3, h=64, w=64)
#           |
#          Conv (in_ch=3, out_ch=32, kernel=3, pads=1)
#           |
#        BatchNorm
#           |
#          ReLU
#           |
#         AvgPool
#           |
#          Conv (in_ch=32, out_ch=10, kernel=1, pads=0)
#           |
#         output (ch=10, h=1, w=1)

if __name__ == "__main__":
    main()

```

è™½ç„¶`onnx`å®˜æ–¹æä¾›äº†è€Œä¸€äº›python APIæ¥ä¿®æ”¹onnxï¼Œä½†æ˜¯æ¨èå¤§å®¶ä½¿ç”¨TensorRTä¸‹çš„`onnxsurgeon`ï¼Œæ›´åŠ æ–¹ä¾¿å¿«æ·ã€‚

* example 3

```python
import onnx_graphsurgeon as gs
import numpy as np
import onnx

# onnx_graph_surgeon(gs)ä¸­çš„IRä¼šæœ‰ä»¥ä¸‹ä¸‰ç§ç»“æ„
# Tensor
#    -- æœ‰ä¸¤ç§ç±»å‹
#       -- Variable:  ä¸»è¦å°±æ˜¯é‚£äº›ä¸åˆ°æ¨ç†ä¸çŸ¥é“çš„å˜é‡
#       -- Constant:  ä¸ç”¨æ¨ç†æ—¶ï¼Œè€Œåœ¨æ¨ç†å‰å°±çŸ¥é“çš„å˜é‡
# Node
#    -- è·Ÿonnxä¸­çš„NodeProtoå·®ä¸å¤š
# Graph
#    -- è·Ÿonnxä¸­çš„GraphProtoå·®ä¸å¤š

def main() -> None:
    input = gs.Variable(
            name  = "input0",
            dtype = np.float32,
            shape = (1, 3, 224, 224))

    weight = gs.Constant(
            name  = "conv1.weight",
            values = np.random.randn(5, 3, 3, 3))

    bias   = gs.Constant(
            name  = "conv1.bias",
            values = np.random.randn(5))
    
    output = gs.Variable(
            name  = "output0",
            dtype = np.float32,
            shape = (1, 5, 224, 224))

    node = gs.Node(
            op      = "Conv",
            inputs  = [input, weight, bias],
            outputs = [output],
            attrs   = {"pads":[1, 1, 1, 1]})

    graph = gs.Graph(
            nodes   = [node],
            inputs  = [input],
            outputs = [output])

    model = gs.export_onnx(graph)

    onnx.save(model, "../models/sample-conv.onnx")



# ä½¿ç”¨onnx.helperåˆ›å»ºä¸€ä¸ªæœ€åŸºæœ¬çš„ConvNet
#         input (ch=3, h=64, w=64)
#           |
#          Conv (in_ch=3, out_ch=32, kernel=3, pads=1)
#           |
#         output (ch=5, h=64, w=64)

if __name__ == "__main__":
    main()


```

### reference

* [https://github.com/open-mmlab/mmdeploy/blob/main/docs/zh\_cn/tutorial/01\_introduction\_to\_model\_deployment.md](https://github.com/open-mmlab/mmdeploy/blob/main/docs/zh\_cn/tutorial/01\_introduction\_to\_model\_deployment.md)
