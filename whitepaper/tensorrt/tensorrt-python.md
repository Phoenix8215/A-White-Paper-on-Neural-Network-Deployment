# ğŸ¦Š TensorRTçš„Pythonæ¥å£è§£æ

æœ¬ç« ä» ONNX æ¨¡å‹å¼€å§‹,è¯´æ˜ Python API çš„åŸºæœ¬ç”¨æ³•ã€‚ [onnx\_resnet50.py](https://github.com/NVIDIA/TensorRT/blob/main/samples/python/introductory\_parser\_samples/onnx\_resnet50.py)ç¤ºä¾‹æ›´è¯¦ç»†åœ°è¯´æ˜äº†è¿™ä¸ªç”¨ä¾‹ã€‚

Python API å¯ä»¥é€šè¿‡tensorrtæ¨¡å—è®¿é—®ï¼š

```python
import tensorrt as trt
```

## The Build Phase

è¦åˆ›å»ºæ„å»ºå™¨ï¼Œæ‚¨éœ€è¦é¦–å…ˆåˆ›å»ºä¸€ä¸ªè®°å½•å™¨ã€‚ Python ç»‘å®šåŒ…æ‹¬ä¸€ä¸ªç®€å•çš„è®°å½•å™¨å®ç°ï¼Œå®ƒå°†é«˜äºæŸä¸€çº§åˆ«çš„æ‰€æœ‰æ¶ˆæ¯è®°å½•åˆ°`stdout` ã€‚

```python
logger = trt.Logger(trt.Logger.WARNING)
```

æˆ–è€…ï¼Œå¯ä»¥é€šè¿‡ä»`ILogger`ç±»æ´¾ç”Ÿæ¥å®šä¹‰è‡ªå·±çš„`logger`å®ç°ï¼š

```python
class MyLogger(trt.ILogger):
    def __init__(self):
       trt.ILogger.__init__(self)

    def log(self, severity, msg):
        pass # Your custom logging implementation here

logger = MyLogger()
```

ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªæ„å»ºå™¨ï¼š

```python
builder = trt.Builder(logger)
```

ç”±äºåˆ›å»ºå¼•æ“æ˜¯ä¸€ä¸ªç¦»çº¿è¿‡ç¨‹ï¼Œå› æ­¤å¯èƒ½éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´ã€‚è¯·å‚é˜… ["ä¼˜åŒ–ç”Ÿæˆå™¨æ€§èƒ½ "](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-861/developer-guide/index.html#opt-builder-perf)éƒ¨åˆ†ï¼Œäº†è§£å¦‚ä½•ä½¿`builder`è¿è¡Œå¾—æ›´å¿«ã€‚

### Creating a Network Definition in Python

åˆ›å»ºç”Ÿæˆå™¨åï¼Œä¼˜åŒ–æ¨¡å‹çš„ç¬¬ä¸€æ­¥å°±æ˜¯åˆ›å»ºç½‘ç»œå®šä¹‰ï¼š

```python
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
```

è¦ä½¿ç”¨ ONNX å¯¼å…¥æ¨¡å‹ï¼Œå¿…é¡»ä½¿ç”¨ `EXPLICIT_BATCH` æ ‡å¿—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… ["æ˜¾å¼æ‰¹å¤„ç†ä¸éšå¼æ‰¹å¤„ç† "](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-861/developer-guide/index.html#explicit-implicit-batch)éƒ¨åˆ†ã€‚

### Importing a Model using the ONNX Parser

ç°åœ¨ï¼Œéœ€è¦ä» ONNX è¡¨ç¤ºä¸­å¡«å……ç½‘ç»œå®šä¹‰ã€‚æ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ª ONNX è§£æå™¨æ¥å¡«å……ç½‘ç»œï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
parser = trt.OnnxParser(network, logger)
```

ç„¶åï¼Œè¯»å–æ¨¡å‹æ–‡ä»¶å¹¶å¤„ç†é”™è¯¯ï¼š

```python
success = parser.parse_from_file(model_path)
for idx in range(parser.num_errors):
    print(parser.get_error(idx))

if not success:
    pass # Error handling code here
```

### Building an Engine

ä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ªé…ç½®ï¼ŒæŒ‡å®š TensorRT åº”è¯¥å¦‚ä½•ä¼˜åŒ–æ¨¡å‹ï¼š

```cpp
config = builder.create_builder_config()
```

è¿™ä¸ªæ¥å£æœ‰å¾ˆå¤šå±æ€§ï¼Œä½ å¯ä»¥è®¾ç½®è¿™äº›å±æ€§æ¥æ§åˆ¶ TensorRT å¦‚ä½•ä¼˜åŒ–ç½‘ç»œã€‚ä¸€ä¸ªé‡è¦çš„å±æ€§æ˜¯æœ€å¤§å·¥ä½œç©ºé—´å¤§å°ã€‚å±‚å®ç°é€šå¸¸éœ€è¦ä¸€ä¸ªä¸´æ—¶å·¥ä½œç©ºé—´ï¼Œå¹¶ä¸”æ­¤å‚æ•°é™åˆ¶äº†ç½‘ç»œä¸­ä»»ä½•å±‚å¯ä»¥ä½¿ç”¨çš„æœ€å¤§å¤§å°ã€‚å¦‚æœæä¾›çš„å·¥ä½œç©ºé—´ä¸è¶³ï¼ŒTensorRT å¯èƒ½æ— æ³•æ‰¾åˆ°å±‚çš„å®ç°ï¼š

```python
config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 20) # 1 MiB
```

æŒ‡å®šé…ç½®åï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ„å»ºå’Œåºåˆ—åŒ–å¼•æ“ï¼š

```python
serialized_engine = builder.build_serialized_network(network, config)
```

å°†å¼•æ“ä¿å­˜åˆ°æ–‡ä»¶ä»¥ä¾›å°†æ¥ä½¿ç”¨ã€‚ä½ å¯ä»¥è¿™æ ·åšï¼š

```python
with open(â€œsample.engineâ€, â€œwbâ€) as f:
    f.write(serialized_engine)
```

**æ³¨æ„ï¼š**åºåˆ—åŒ–å¼•æ“ä¸èƒ½è·¨å¹³å°ç§»æ¤ã€‚é™¤å¹³å°å¤–ï¼Œå¼•æ“è¿˜ä¸æ‰€æ„å»ºçš„ GPU å‹å·ç›¸å…³ã€‚

## Deserializing a Plan

è¦æ‰§è¡Œæ¨ç†ï¼Œé¦–å…ˆéœ€è¦ä½¿ç”¨Runtimeæ¥å£ååºåˆ—åŒ–å¼•æ“ã€‚ä¸æ„å»ºå™¨ä¸€æ ·ï¼Œè¿è¡Œæ—¶éœ€è¦loggerçš„å®ä¾‹ã€‚

```python
runtime = trt.Runtime(logger)
```

ç„¶åï¼Œå¯ä»¥ä»å†…å­˜ç¼“å†²åŒºååºåˆ—åŒ–å¼•æ“ï¼š

```python
engine = runtime.deserialize_cuda_engine(serialized_engine)
```

å¦‚æœéœ€è¦é¦–å…ˆä»æ–‡ä»¶åŠ è½½å¼•æ“ï¼Œè¯·è¿è¡Œï¼š

```python
with open(â€œsample.engineâ€, â€œrbâ€) as f:
    serialized_engine = f.read()
```

## Performing Inference

å¼•æ“æ‹¥æœ‰ä¼˜åŒ–åçš„æ¨¡å‹ï¼Œä½†è¦æ‰§è¡Œæ¨ç†éœ€è¦é¢å¤–çš„ä¸­é—´æ¿€æ´»çŠ¶æ€ã€‚è¿™æ˜¯é€šè¿‡`IExecutionContext`æ¥å£å®Œæˆçš„ï¼š

```python
context = engine.create_execution_context()
```

ä¸€ä¸ªå¼•æ“å¯ä»¥æœ‰å¤šä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå…è®¸ä¸€ç»„æƒé‡ç”¨äºå¤šä¸ªé‡å çš„æ¨ç†ä»»åŠ¡ã€‚ ï¼ˆä¸€ä¸ªä¾‹å¤–æ˜¯ä½¿ç”¨åŠ¨æ€å½¢çŠ¶æ—¶ï¼Œæ¯ä¸ªä¼˜åŒ–é…ç½®æ–‡ä»¶åªèƒ½æœ‰ä¸€ä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡ã€‚ï¼‰

è¦æ‰§è¡Œæ¨ç†ï¼Œå¿…é¡»ä¸ºè¾“å…¥å’Œè¾“å‡ºæŒ‡å®šç¼“å†²åŒºï¼šï¼š

```python
context.set_tensor_address(name, ptr)
```

æœ‰å‡ ä¸ª Python è½¯ä»¶åŒ…å…è®¸åœ¨ GPU ä¸Šåˆ†é…å†…å­˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå®˜æ–¹çš„ CUDA Python ã€PyTorchã€cuPy å’Œ Numbaã€‚

å¡«å……è¾“å…¥ç¼“å†²åŒºåï¼Œå¯ä»¥è°ƒç”¨ TensorRT çš„ `execute_async_v3` æ–¹æ³•ï¼Œå¼€å§‹ä½¿ç”¨ CUDA æµè¿›è¡Œæ¨ç†ã€‚ç½‘ç»œæ˜¯å¦ä»¥å¼‚æ­¥æ–¹å¼æ‰§è¡Œå–å†³äºç½‘ç»œçš„ç»“æ„å’Œç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œæ•°æ®çš„ç»´åº¦ã€DLA ä½¿ç”¨ã€å¾ªç¯å’ŒåŒæ­¥æ’ä»¶ç­‰éƒ½å¯èƒ½å¯¼è‡´åŒæ­¥è¡Œä¸ºã€‚

é¦–å…ˆï¼Œåˆ›å»º CUDA æ•°æ®æµã€‚å¦‚æœå·²ç»æœ‰ä¸€ä¸ª CUDA æµï¼Œåˆ™å¯ä»¥ä½¿ç”¨æŒ‡å‘ç°æœ‰æµçš„æŒ‡é’ˆã€‚ä¾‹å¦‚ï¼Œå¯¹äº PyTorch CUDA æµï¼ˆå³ `torch.cuda.Stream()`ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ `cuda_stream` å±æ€§è®¿é—®æŒ‡é’ˆï¼›å¯¹äº Polygraphy CUDA æµï¼Œå¯ä»¥ä½¿ç”¨ ptr å±æ€§ï¼›ä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨ `cudaStreamCreate()` ç›´æ¥ä½¿ç”¨ CUDA Python ç»‘å®šåˆ›å»ºæµã€‚æ¥ä¸‹æ¥ï¼Œå¼€å§‹æ¨ç†ï¼š

```python
context.execute_async_v3(buffers, stream_ptr)
```

åœ¨å†…æ ¸ä¹‹å‰å’Œä¹‹åå¯åŠ¨å¼‚æ­¥ä¼ è¾“ï¼ˆ`cudaMemcpyAsync()`ï¼‰æ˜¯å¾ˆå¸¸è§çš„åšæ³•ï¼Œä»¥ä¾¿ä» GPU è½¬ç§»æ•°æ®ã€‚

è¦ç¡®å®šæ¨ç†ï¼ˆå’Œå¼‚æ­¥ä¼ è¾“ï¼‰ä½•æ—¶å®Œæˆï¼Œå¯ä½¿ç”¨æ ‡å‡†çš„ CUDA åŒæ­¥æœºåˆ¶ï¼Œå¦‚äº‹ä»¶æˆ–åœ¨æµä¸Šç­‰å¾…ã€‚ä¾‹å¦‚ï¼Œå¯¹äº PyTorch CUDA æµæˆ– Polygraphy CUDA æµï¼Œå¯å‘å‡º `stream.synchronize()`ã€‚å¯¹äºä½¿ç”¨ CUDA Python åˆ›å»ºçš„æµï¼Œè¯·æ‰§è¡Œ `cudaStreamSynchronize(stream)`ã€‚
