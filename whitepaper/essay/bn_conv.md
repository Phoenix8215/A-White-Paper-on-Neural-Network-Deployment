# ğŸƒ èåˆBNå’ŒConvå±‚

å±‚èåˆå¯ä»¥å‡å°‘å¯åŠ¨kernelçš„å¼€é”€ä¸memoryæ“ä½œï¼Œä»è€Œæé«˜æ•ˆç‡ åŒæ—¶ï¼Œæœ‰äº›è®¡ç®—å¯ä»¥é€šè¿‡å±‚èåˆä¼˜åŒ–åï¼Œè·Ÿå…¶ä»–è®¡ç®—åˆå¹¶

* `Vertical layer fusion` (å‚ç›´å±‚èåˆ)ï¼šç”¨çš„æ¯”è¾ƒå¸¸è§ï¼Œé’ˆå¯¹`conv + BN + ReLU`è¿›è¡Œèåˆ

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (52).png" alt=""><figcaption></figcaption></figure>

* `Horizontal layer fusion` (æ°´å¹³å±‚èåˆ)

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (53).png" alt=""><figcaption></figcaption></figure>

å›é¡¾ä¸€ä¸‹`Batch Normalization`çš„å…¬å¼,å…¶ä¸­

<figure><img src="../../.gitbook/assets/å›¾ç‰‡ (54).png" alt="" width="339"><figcaption></figcaption></figure>

$$
\begin{aligned}\mu_B&=\frac{1}{B}\sum_{i=1}^{B}x_i\\\sigma_B^2&=\frac{1}{B}\sum_{i=1}^{B}(x_i\mu_B)^2+\epsilon\end{aligned}\begin{aligned}\widehat{x_i}&=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}     \quad  \\     \quad y_i&=\gamma*\widehat{x_i}+\beta\end{aligned}\xrightarrow{\text{å±•å¼€}} \quad y _ i = \gamma * \frac { x _ i - \mu _ B }{ \sqrt { \sigma _ B ^ 2 + \epsilon }}+\beta \frac{\text{ä»£å…¥}}{x_i=w*x+b}\quad y=\gamma*\frac{w*x+b-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}+\beta
$$

$$
\begin{aligned}
&y=\gamma*\frac{w*x+b-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}}+\beta  \\
&y=\frac{\gamma^{*}w}{\sqrt{\sigma_{B}^{2}+\epsilon}}*x+\frac{\gamma}{\sqrt{\sigma_{B}^{2}+\epsilon}}(b-\mu_{B})+\beta  \\
&y=(\frac{\gamma*w}{\sqrt{\sigma_{B}^{2}+\epsilon}})*x+(\frac{\gamma}{\sqrt{\sigma_{B}^{2}+\epsilon}}(b-\mu_{B})+\beta)
\end{aligned}
$$

$$
\begin{array}{l}\widehat{w}=\dfrac{\gamma*w}{\sqrt{\sigma_B^2+\epsilon}}\\\widehat{b}=\dfrac{\gamma}{\sqrt{\sigma_B^2+\epsilon}}(b-\mu_B)+\beta\end{array}
$$

è¿™ä¸¤ä¸ªå‚æ•°å€¼å¯ä»¥æå‰è®¡ç®—å‡ºæ¥.

{% hint style="info" %}
å¾ˆå¤šæ¨¡å‹ç»å¸¸ä¼šæœ‰å¾ˆå¤š ç§ç±»çš„activation functionï¼Œæ¯”å¦‚ GELU, Swish, Mishç­‰ç­‰ï¼Œè¿™äº›æ¿€æ´»å‡½æ•°å¾€å¾€ç”±äºè®¡ç®—å¤æ‚å¾ˆéš¾åŠ é€Ÿï¼Œå¯ä»¥å°è¯•æ”¹æˆReLUçœ‹çœ‹ç²¾åº¦çš„æ”¹å˜å’Œæ€§èƒ½çš„æå‡.
{% endhint %}

æŠŠä¸Šé¢çš„å…¬å¼å†™æˆçŸ©é˜µçš„å½¢å¼ï¼š

$$
å¯¹äºä¸€ä¸ªå½¢çŠ¶ä¸ºC\times H\times Wçš„ç‰¹å¾å›¾F,è®°å½’ä¸€åŒ–ç»“æœ\hat{F},è®¡ç®—å¦‚ä¸‹:
$$

$$
\begin{aligned}&\begin{pmatrix}\hat{F}_{1,i,j}\\\hat{F}_{2,i,j}\\\vdots\\\hat{F}_{C-1,i,j}\\\hat{F}_{C,i,j}\end{pmatrix}=\begin{pmatrix}\frac{\gamma_1}{\sqrt{\sigma_1^2+\epsilon}}&0&\cdots&0\\0&\frac{\gamma_2}{\sqrt{\sigma_2^2+\epsilon}}\\\vdots&&\ddots&\vdots\\&&\frac{\gamma_{C-1}}{\sqrt{\sigma_{C-1}^2+\epsilon}}&0\\0&\cdots&0&\frac{\gamma_C}{\sqrt{\sigma_C^2+\epsilon}}\end{pmatrix}\cdot\begin{pmatrix}F_{1,i,j}\\F_{2,i,j}\\\vdots\\F_{C-1,i,j}\\F_{C,i,j}\end{pmatrix}+\begin{pmatrix}\beta_1-\gamma_1\frac{\hat{\mu}_1}{\sqrt{\sigma_1^2+\epsilon}}\\\beta_2-\gamma_2\frac{\hat{\mu}_2}{\sqrt{\sigma_2^2+\epsilon}}\\\vdots\\\beta_{C-1}-\gamma_{C-1}\frac{\hat{\mu}_C^2+\epsilon}{\sqrt{\sigma_C^2+\epsilon}}\end{pmatrix}\end{aligned}
$$

ä»£ç å¦‚ä¸‹ï¼š

```python
def fuse_conv_and_bn(conv, bn):
	#
	# init
	fusedconv = torch.nn.Conv2d(
		conv.in_channels,
		conv.out_channels,
		kernel_size=conv.kernel_size,
		stride=conv.stride,
		padding=conv.padding,
		bias=True
	)
	#
	# prepare filters
	w_conv = conv.weight.clone().view(conv.out_channels, -1)
	w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))
	fusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )
	#
	# prepare spatial bias
	if conv.bias is not None:
		b_conv = conv.bias
	else:
		b_conv = torch.zeros( conv.weight.size(0) )
	b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
	fusedconv.bias.copy_( torch.matmul(w_bn, b_conv) + b_bn )
	#
	# we're done
	return fusedconv

if __name__ == "__main__":
    import torch
    import torchvision

    torch.set_grad_enabled(False)
    x = torch.randn(16, 3, 256, 256)
    rn18 = torchvision.models.resnet18(pretrained=True)
    rn18.eval()
    net = torch.nn.Sequential(
        rn18.conv1,
        rn18.bn1
    )
    y1 = net.forward(x)
    fusedconv = fuse_conv_and_bn(net[0], net[1])
    y2 = fusedconv.forward(x)
    d = (y1 - y2).norm().div(y1.norm()).item()
    print("error: %.8f" % d)
# error: 0.00000030
```
